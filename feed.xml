<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://lialittis.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://lialittis.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-08-20T14:08:44+00:00</updated><id>https://lialittis.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Performance Benefits From Underutilized CPU</title><link href="https://lialittis.github.io/blog/2023/Performance-Benefits-From-Underutilized-CPU/" rel="alternate" type="text/html" title="Performance Benefits From Underutilized CPU"/><published>2023-08-18T00:30:00+00:00</published><updated>2023-08-18T00:30:00+00:00</updated><id>https://lialittis.github.io/blog/2023/Performance%20Benefits%20From%20Underutilized%20CPU</id><content type="html" xml:base="https://lialittis.github.io/blog/2023/Performance-Benefits-From-Underutilized-CPU/"><![CDATA[]]></content><author><name></name></author><category term="performance"/><category term="security"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">How to Write A CORRECT LLVM Pass As A Plugin</title><link href="https://lialittis.github.io/blog/2023/How-to-Write-A-CORRECT-LLVM-Pass-As-A-Plugin/" rel="alternate" type="text/html" title="How to Write A CORRECT LLVM Pass As A Plugin"/><published>2023-08-06T00:30:00+00:00</published><updated>2023-08-06T00:30:00+00:00</updated><id>https://lialittis.github.io/blog/2023/How%20to%20Write%20A%20CORRECT%20LLVM%20Pass%20As%20A%20Plugin</id><content type="html" xml:base="https://lialittis.github.io/blog/2023/How-to-Write-A-CORRECT-LLVM-Pass-As-A-Plugin/"><![CDATA[]]></content><author><name></name></author><category term="compiler"/><category term="llvm"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">What is Three Address Code (3AC) ?</title><link href="https://lialittis.github.io/blog/2023/What-is-Three-Address-Code/" rel="alternate" type="text/html" title="What is Three Address Code (3AC) ?"/><published>2023-08-02T00:20:00+00:00</published><updated>2023-08-02T00:20:00+00:00</updated><id>https://lialittis.github.io/blog/2023/What%20is%20Three%20Address%20Code</id><content type="html" xml:base="https://lialittis.github.io/blog/2023/What-is-Three-Address-Code/"><![CDATA[]]></content><author><name></name></author><category term="compiler"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Lexical Analysis, Syntax Analysis, and Semantic Analysis</title><link href="https://lialittis.github.io/blog/2023/Lexical-Analysis,-Syntax-Analysis,-and-Semantic-Analysis-copy/" rel="alternate" type="text/html" title="Lexical Analysis, Syntax Analysis, and Semantic Analysis"/><published>2023-08-01T17:39:00+00:00</published><updated>2023-08-01T17:39:00+00:00</updated><id>https://lialittis.github.io/blog/2023/Lexical%20Analysis,%20Syntax%20Analysis,%20and%20Semantic%20Analysis%20copy</id><content type="html" xml:base="https://lialittis.github.io/blog/2023/Lexical-Analysis,-Syntax-Analysis,-and-Semantic-Analysis-copy/"><![CDATA[]]></content><author><name></name></author><category term="compiler"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Rust Basics 1(Quick Review)</title><link href="https://lialittis.github.io/blog/2023/Rust-Basics-1(Quick-Review)/" rel="alternate" type="text/html" title="Rust Basics 1(Quick Review)"/><published>2023-07-01T00:00:00+00:00</published><updated>2023-07-01T00:00:00+00:00</updated><id>https://lialittis.github.io/blog/2023/Rust%20Basics%201(Quick%20Review)</id><content type="html" xml:base="https://lialittis.github.io/blog/2023/Rust-Basics-1(Quick-Review)/"><![CDATA[<h1 id="rust-basics1">Rust Basics(1)</h1> <h2 id="anatomy-of-a-rust-program">Anatomy of a Rust Program</h2> <ul> <li>indent with four spaces, not a tab</li> <li><code class="language-plaintext highlighter-rouge">println!</code> calls a Rust marco</li> <li>end the line with a semicolon(;)</li> </ul> <h2 id="compiling-and-running-are-seperate-steps">Compiling and Running Are Seperate Steps</h2> <p>Rust is an <em>ahead-of-time</em> compiled language.</p> <h2 id="cargo">Cargo</h2> <p>Cargo creats a project in a new directory and initializes a new Git repository along with a <code class="language-plaintext highlighter-rouge">.gitignore</code> file.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Note: Git is a common version control system. You can change cargo new to use a different version control 
system or no version control system by using the --vcs flag. Run cargo new --help to see the available 
options.
</code></pre></div></div> <p>Cargo expects sources files to live inside the <code class="language-plaintext highlighter-rouge">src</code> directory and puts the binary in a directory named <code class="language-plaintext highlighter-rouge">debug</code>.</p> <ul> <li><code class="language-plaintext highlighter-rouge">cargo new</code></li> <li><code class="language-plaintext highlighter-rouge">cargo build</code></li> <li><code class="language-plaintext highlighter-rouge">cargo run</code></li> <li><code class="language-plaintext highlighter-rouge">cargo check</code></li> </ul> <h3 id="tomltoms-obvious-minimal-language">TOML(Tom’s Obvious, Minimal Language)</h3> <p>It is Cargo’s configuration format.</p> <h3 id="building-for-release">Building for Release</h3> <p><code class="language-plaintext highlighter-rouge">cargo build --release</code> compile the project with optimizations.</p> <h2 id="crate">Crate</h2> <p>A <code class="language-plaintext highlighter-rouge">crate</code> is a collection of Rust source code files. For example, the <code class="language-plaintext highlighter-rouge">rand</code> crate is a <em>library</em> crate, which contains code that is intended to be used in other programs.</p> <h3 id="external-crates">external crates</h3> <p>The external crates are added into projects as dependencies. Cargo fetches the latest versions of everything that dependency needs from the <em>registry</em>, which is a copy of data from <code class="language-plaintext highlighter-rouge">Crates.io</code>.</p> <h3 id="cargolock">Cargo.lock</h3> <p>If <code class="language-plaintext highlighter-rouge">Cargo.lock</code> exists, Cargo will use the versions specified there rather than doing all the work of figuring out versions again, which lets us have a reproducible build automatically.</p> <h3 id="update">update</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cargo update
</code></pre></div></div> <p>helps you to update a crate.</p> <h1 id="common-programming-concepts">Common Programming Concepts</h1> <h2 id="variables-and-mutability">Variables and Mutability</h2> <p>By default, variables in Rust are immutable.</p> <p>You would get compile-time errors when we attempt to change a value that is designated as immutable.</p> <h3 id="constants">Constants</h3> <p>Like immutable variables, but constants:</p> <ul> <li>don’t accept mut syntax.</li> <li>use const keyword</li> <li>the type of the value must be annotated</li> </ul> <h3 id="shadowing">Shadowing</h3> <div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">fn</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
    <span class="k">let</span> <span class="n">x</span> <span class="o">=</span> <span class="mi">5</span><span class="p">;</span>

    <span class="k">let</span> <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span>

    <span class="p">{</span>
        <span class="k">let</span> <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span><span class="p">;</span>
        <span class="nd">println!</span><span class="p">(</span><span class="s">"The value of x in the inner scope is: {x}"</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="nd">println!</span><span class="p">(</span><span class="s">"The value of x is: {x}"</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div> <p>When you run it:</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>cargo run
   Compiling variables v0.1.0 <span class="o">(</span>file:///projects/variables<span class="o">)</span>
    Finished dev <span class="o">[</span>unoptimized + debuginfo] target<span class="o">(</span>s<span class="o">)</span> <span class="k">in </span>0.31s
     Running <span class="sb">`</span>target/debug/variables<span class="sb">`</span>
The value of x <span class="k">in </span>the inner scope is: 12
The value of x is: 6
</code></pre></div></div> <p>But shadowing:</p> <ul> <li>we create a new variable</li> <li>we can change the type of the value</li> </ul> <h2 id="data-types">Data Types</h2> <p>Rust is a statically typed language, it must know the types of all variables at compile time.</p> <h3 id="scalar-types">Scalar Types</h3> <p>Four primary scalar types : integers, floating-point numbers, Booleans and characters.</p> <h4 id="integer-types">Integer Types</h4> <p>Integer Literals in Rust</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Number literals	Example
Decimal	98_222
Hex	0xff
Octal	0o77
Binary	0b1111_0000
Byte (u8 only)	b'A'
</code></pre></div></div> <p>Integer Overflow</p> <ul> <li>In debug mode, Rust includes checks for integer overflow and cause the program to <code class="language-plaintext highlighter-rouge">panic</code> at runtime.</li> <li>In release mode, Rust performs <em>two’s complement wrapping</em> for the value, and the program won’t panic.</li> </ul> <h4 id="floating-point-types">Floating-point Types</h4> <p>Two primitive types : numbers with decimal points f32 and f64.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>fn main() {
    let x = 2.0; // f64

    let y: f32 = 3.0; // f32
}
</code></pre></div></div> <h3 id="boolean-type">Boolean Type</h3> <p><code class="language-plaintext highlighter-rouge">true</code> and <code class="language-plaintext highlighter-rouge">false</code></p> <h3 id="character-type">Character Type</h3> <p>Four bytes in size and Unicode Scalar Value.</p> <h3 id="compound-types">Compound Types</h3> <h4 id="tuple">Tuple</h4> <ul> <li>a variety fo types</li> <li>a fixed length</li> <li>comma-separated list</li> <li>access a tuple element by using a period (.) followed by the index</li> <li>tuple without any values has a special name, <em>unit</em> : an empty value ()</li> </ul> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>fn main() {
    let tup: (i32, f64, u8) = (500, 6.4, 1);
}
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>fn main() {
    let tup = (500, 6.4, 1);

    let (x, y, z) = tup;

    println!("The value of y is: {y}");
}
</code></pre></div></div> <h4 id="array">Array</h4> <ul> <li>same type</li> <li>a fixed lenght</li> <li>comma-separated list</li> <li>access elements by using indexing</li> </ul> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>fn main() {
    let a = [1, 2, 3, 4, 5];
    
    let a: [i32; 5] = [1, 2, 3, 4, 5];

    let a = [3; 5]; // equal to [3, 3, 3, 3, 3];

    let first = a[0];
    let second = a[1];

}
</code></pre></div></div> <p>Arrays are useful when you want your data allocated on the stack rather than the heap.</p> <p><strong>Runtime error : invalid array element access:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>use std::io;

fn main() {
    let a = [1, 2, 3, 4, 5];

    println!("Please enter an array index.");

    let mut index = String::new();

    io::stdin()
        .read_line(&amp;mut index)
        .expect("Failed to read line");

    let index: usize = index
        .trim()
        .parse()
        .expect("Index entered was not a number");

    let element = a[index];

    println!("The value of the element at index {index} is: {element}");
}
</code></pre></div></div> <p>The program resulted in a runtime error at the point of using an invalid value in the indexing operation. This check has to happen at runtime, especially in this case, because the compiler can’t possibly know what value a user will enter when they run the code later.</p> <h2 id="functions">Functions</h2> <p>Functions are prevalent in Rust code.</p> <p>Rust code uses <em>snake case</em> as the conventional style for function and variable names.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>fn main() {
    println!("Hello, world!");

    another_function();
}

fn another_function() {
    println!("Another function.");
}
</code></pre></div></div> <p>In function signatures, you must declare the type of each parameter. <strong>Requiring type annotations in function definitions means the compiler almost never needs you to use them elsewhere in the code to figure out what type you mean</strong>.</p> <h3 id="statements-and-expressions">Statements and Expressions</h3> <p>Function bodies are made up of a series of statements optionally ending in an expression.</p> <ul> <li>Statements are instructions that perform some action and do not return a value.</li> <li>Expressions evaluate to a resultant value.</li> </ul> <p>Expressions do not include ending semicolons. If you add a semicolon to the end of an expression, you turn it into a statement, and it will then not return a value.</p> <h3 id="functions-with-return-values">Functions with Return Values</h3> <p>We must declare the return type after an arrow (-&gt;).</p> <p>We need to have an expression as the return value. If not, there might be mismatched types error.</p> <h2 id="control-flow">Control Flow</h2> <h3 id="if-expressions">if Expressions</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>if condition {

} else {

}
</code></pre></div></div> <p>Boolean is expected after <code class="language-plaintext highlighter-rouge">if</code>, Rust cannot automatically convert non-Boolean to Boolean types.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>if condition {

} else if condition {

} ... {

} else {

}
</code></pre></div></div> <p>Using too many <code class="language-plaintext highlighter-rouge">else if</code> expressions can clutter your code, so if you have more that one, you might wnat to refactor your code. For example by <code class="language-plaintext highlighter-rouge">match</code>.</p> <h4 id="using-if-in-a-let-statement">Using if in a let Statement</h4> <p>Because <code class="language-plaintext highlighter-rouge">if</code> is an expression, it could be used on the right side of a <code class="language-plaintext highlighter-rouge">let</code> statement.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>fn main() {
    let condition = true;
    let number = if condition { 5 } else { 6 };

    println!("The value of number is: {number}");
}
</code></pre></div></div> <h3 id="repetition-with-loops">Repetition with Loops</h3> <p>Loop is an expression.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>fn main() {
    loop {
        println!("again!");
    }
}
</code></pre></div></div> <h4 id="returning-values-from-loops">Returning Values from Loops</h4> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>fn main() {
    let mut counter = 0;

    let result = loop {
        counter += 1;

        if counter == 10 {
            break counter * 2;
        }
    };

    println!("The result is {result}");
}
</code></pre></div></div> <h4 id="loop-labels-to-disambiguate-between-multiple-loops">Loop Labels to Disambiguate Between Multiple Loops</h4> <p>We could specify a loop label so that we use with <code class="language-plaintext highlighter-rouge">break</code> or <code class="language-plaintext highlighter-rouge">continue</code> to specify that those keywords apply to the labeled loop instead of the innermost loop.</p> <p>Note that, the <code class="language-plaintext highlighter-rouge">break</code> keyword cannot be followed by both a return value and a loop label at the same time. The <code class="language-plaintext highlighter-rouge">break</code> statement is used to exit a loop or a block early, and it can be optionally followed by an expression to provide a return value for the enclosing function or block. However, it cannot be combined with a loop label.</p> <p>The labels should be in format like : <code class="language-plaintext highlighter-rouge">'lable</code>.</p> <h4 id="conditional-loops-with-while">Conditional Loops with while</h4> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>fn main() {
    let mut number = 3;

    while number != 0 {
        println!("{number}!");

        number -= 1;
    }

    println!("LIFTOFF!!!");
}
</code></pre></div></div> <h4 id="looping-through-a-collection-with-for">Looping Through a Collection with for</h4> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>fn main() {
    let a = [10, 20, 30, 40, 50];

    for element in a {
        println!("the value is: {element}");
    }
}
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>fn main() {
    for number in (1..4).rev() {
        println!("{number}!");
    }
    println!("LIFTOFF!!!");
}
</code></pre></div></div>]]></content><author><name>TC YU</name></author><category term="Tutorials"/><category term="Rust"/><summary type="html"><![CDATA[Rust Basics(1)]]></summary></entry><entry><title type="html">Signature Algorithm 数字签名算法</title><link href="https://lialittis.github.io/blog/2020/Signature-Algorithm/" rel="alternate" type="text/html" title="Signature Algorithm 数字签名算法"/><published>2020-11-16T00:00:00+00:00</published><updated>2020-11-16T00:00:00+00:00</updated><id>https://lialittis.github.io/blog/2020/Signature%20Algorithm</id><content type="html" xml:base="https://lialittis.github.io/blog/2020/Signature-Algorithm/"><![CDATA[<h1 id="signature-algorithm">Signature algorithm</h1> <h2 id="definitions-and-properties">Definitions and properties</h2> <h3 id="definitions">Definitions</h3> <p><strong>Required properties for a signature</strong></p> <ul> <li>authentic</li> <li>unforgeable</li> <li>cannot be reused</li> <li>immutable</li> </ul> <p><strong>In the asymmetric world</strong></p> <ul> <li>non repudiation</li> <li>verifiable</li> </ul> <h3 id="properties">Properties</h3> <p><img src="" alt="signature &amp; verification"/></p> <p>Choosing a system:</p> <ul> <li>signing must be easy for the legitimate user (trapdoor)【合法用户】 and impossible for anybody else;</li> <li>verification can be done easily.</li> </ul> <p>Duality with ((E,$k_e$),(D,$k_d$)) in asymmetric encryption.</p> <h3 id="security-issues">Security issues</h3> <ul> <li>Types of forgery <ul> <li>total break</li> <li>selective forgery</li> <li>existential forgery</li> </ul> </li> <li>Types of attacks <ul> <li>key-only</li> <li>message attacks <ul> <li>known_message attack</li> <li>chosen-message attack</li> <li>adaptive chosen-message attack</li> </ul> </li> </ul> </li> </ul> <h2 id="signatures-in-the-asymmetric-world">Signatures in the asymmetric world</h2> <h3 id="a-signature-with-appendix"><strong>A. Signature with appendix</strong></h3> <p><em>Prerequisite</em>: each user has a pair (S, V) where S is the private signature algorithm, V the public verification algorithm, s.t. V(m, S(m)) =true.</p> <p><em>Signature</em>: Alice signs m and sends (m, $S_A$(m)).</p> <p><em>Verification</em>: Bob gets an authenticated copy of Alice’s $V_A$ and tests whether $V_A$(m,s) ==true.</p> <p>Rem.</p> <ul> <li>m is necessary for the verification</li> <li>if m is too long, one uses $S(m) = S’(\mathcal{H}(m))$</li> </ul> <p>Ex.(typical) Alice has an RSA key $(N_A,e_A,d_A)$; $S_A(m) = m^{d_A} mod N_A$; $V_A(m,s) = (s^{e_A} mod N_A == m)$.</p> <p><strong>But</strong>:(E(x),x) is a valid pair =&gt; one must not sign everything; a MAC can be added. ????????????</p> <h3 id="b-schemes-with-message-recovery"><strong>B. Schemes with message recovery</strong></h3> <p>Idea: S(m) yields m, which increases the bandwidth.</p> <p>Ex. (toy) $S_A$(m) = m^{dA} mod $N_A$, $V_A$(s) = s^{$e_A$} mod $N_A$; if E(D(m)) = m, one takes S = D, V = E.</p> <p>But, since x is a signature on E(x) (due to V(x) = E(x)); =&gt; one must be able to recognize a genuine message, i.e., formatted using a redundancy function R.</p> <table> <tbody> <tr> <td>Ex. R(m) = m</td> <td> </td> <td>m: a random m’ possesses the good redundancy with proba $2^{-n}$.</td> </tr> </tbody> </table> <ul> <li>Signature: Alice computes m’ = R(m) and sends s = S_A(m’)</li> <li>Verification : Bob gets the authenticate verification algorithm of Alice; Bob computes m’’ = V_A(s) and checks that m’’ show the required redundancy: if yes, he gets back m as $R^{-1}(m’’)$;otherwise, the signature is rejected.</li> </ul> <h2 id="signing-with-rsa">Signing with RSA</h2> <h3 id="with-appendix">with appendix</h3> <p>First idea: TEXTBOOK-RSA, but possible attacks using the malleability property.</p> <p>Second idea: $S(m) = \mathcal{H}(m)^d mod N$ with $\mathcal{H} = MD5$; $V(m,s) = ((s^e mod N) == \mathcal{H}(m))$</p> <p>Desmedt-Odlyzko; Coron-Naccache-Stern 【待补充】</p> <ul> <li><strong>PSS</strong></li> </ul> <p>Probabilistic signature scheme (Bellare Rogaway) with security proof</p> <p>Prerequisite:</p> <ul> <li><strong>with message recovery</strong></li> </ul> <table> <tbody> <tr> <td>simple idea: $R(m) = mw = m</td> <td> </td> <td>0…0; k = log2N + 1 ; t&lt;k/2, w = 2^t and 0 \leq m &lt; N/w -1$</td> </tr> </tbody> </table> <h2 id="elgamal-and-variants">ElGamal and variants</h2> <h2 id="signcrypt">SignCrypt</h2> <h2 id="中文部分注解">中文部分注解</h2> <ol> <li>签名具有不可伪造性，签名者可以用它很好地证明自己的身份。同时，前面的另一个重要的功能就是，它可以使被签名的文件局游法律效应，签名者无法否认自己签过的文件。——“抗抵赖性”</li> <li>带有附录的签名方式，是指原信息也作为签名算法的输出的一部分，签名作为整个输出信息的附录而存在的，因此验证算法中需要原信息和签名一同作为输入。这种签名算法与信息恢复（message recovery）签名算法相反，后者往往在签名中嵌入信息，当所有的信息都被嵌入时，验证程序只需要将签名作为输入，然后将信息恢复（类似与副产品一样）。</li> <li><strong>ElGamal加密算法</strong> : ElGamal加密算法是一个基于迪菲-赫尔曼密钥交换的非对称加密算法。它在1985年由塔希尔·盖莫尔提出。GnuPG和PGP等很多密码学系统中都应用到了ElGamal算法。ElGamal加密算法可以定义在任何循环群G上。它的安全性取决于G上的离散对数难题。ElGamal加密算法由三部分组成：密钥生成、加密和解密。</li> </ol> <p>A. 密钥生成</p> <p>密钥生成的步骤如下：</p> <ul> <li>Alice利用生成元g产生一个q\,阶循环群G\,的有效描述。该循环群需要满足一定的安全性质。</li> <li>Alice从$\displaystyle {1,\ldots ,q-1}$中随机选择一个x。</li> <li>Alice计算${\displaystyle h:=g^{x}}$。</li> <li>Alice公开h\,以及{\displaystyle G,q,g\,}的描述作为其公钥，并保留x作为其私钥。私钥必须保密。</li> </ul> <p>加密</p> <p>使用Alice的公钥{\displaystyle (G,q,g,h)}向她加密一条消息m的加密算法工作方式如下：</p> <ul> <li>Bob从${\displaystyle {1,\ldots ,q-1}}$随机选择一个y，然后计算${\displaystyle c_{1}:=g^{y}}$。</li> <li>Bob计算共享秘密{\displaystyle s:=h^{y}}。</li> <li>Bob把他要发送的秘密消息m映射为G上的一个元素{\displaystyle m’}。</li> <li>Bob计算{\displaystyle c_{2}:=m’\cdot s}。</li> <li>Bob将密文{\displaystyle (c_{1},c_{2})=(g^{y},m’\cdot h^{y})=(g^{y},m’\cdot (g^{x})^{y})}发送给Alice。 值得注意的是，如果一个人知道了{\displaystyle m’}，那么它很容易就能知道{\displaystyle h^{y}}的值。因此对每一条信息都产生一个新的y可以提高安全性。所以y也被称作临时密钥（英语：ephemeral key）。</li> </ul> <p>解密 利用私钥x对密文{\displaystyle (c_{1},c_{2})}进行解密的算法工作方式如下：</p> <p>Alice计算共享秘密{\displaystyle s:=c_{1}{}^{x}} 然后计算{\displaystyle m’:=c_{2}\cdot s^{-1}}，并将其映射回明文m，其中{\displaystyle s^{-1}}是s在群G上的逆元。（例如：如果G是整数模n乘法群的一个子群，那么逆元就是模逆元）。 解密算法是能够正确解密出明文的，因为 {\displaystyle c_{2}\cdot s^{-1}=m’\cdot h^{y}\cdot (g^{xy})^{-1}=m’\cdot g^{xy}\cdot g^{-xy}=m’.} 实际使用 ElGamal加密系统通常应用在混合加密系统（英语：hybrid cryptosystem）中。例如：用对称加密体制来加密消息，然后利用ElGamal加密算法传递密钥。这是因为在同等安全等级下，ElGamal加密算法作为一种非对称密码学系统，通常比对称加密体制要慢。对称加密算法的密钥和要传递的消息相比通常要短得多，所以相比之下使用ElGamal加密密钥然后用对称加密来加密任意长度的消息，这样要更快一些。</p> <ol> <li>Diffie-Hellman</li> </ol> <p>Diffie-Hellman 密钥交换</p>]]></content><author><name>TC YU</name></author><category term="Cryptography"/><category term="密码学"/><category term="Signature Algorithm"/><summary type="html"><![CDATA[Signature algorithm]]></summary></entry><entry><title type="html">CS423 Distributed Information Systems</title><link href="https://lialittis.github.io/blog/2020/CS423-Distributed-Information-Systems/" rel="alternate" type="text/html" title="CS423 Distributed Information Systems"/><published>2020-08-30T00:00:00+00:00</published><updated>2020-08-30T00:00:00+00:00</updated><id>https://lialittis.github.io/blog/2020/CS423%20Distributed%20Information%20Systems</id><content type="html" xml:base="https://lialittis.github.io/blog/2020/CS423-Distributed-Information-Systems/"><![CDATA[\[\def\abs#1{\left|\,#1\,\right|}\] <h1 id="cs423">CS423</h1> <blockquote> <p><a href="https://github.com/zifeo/EPFL/blob/master/LICENSE">GNU General Public License v3.0</a> licensed. Source available on <a href="https://github.com/zifeo/EPFL">github.com/zifeo/EPFL</a>.</p> </blockquote> <p>Spring 2017: Distributed Information Systems</p> <p>[TOC]</p> <h2 id="0-overview">0 Overview</h2> <h3 id="information-system">Information system</h3> <ul> <li>information system : software that manages a model of some aspect of real world within a distributed computer system for a given purpose</li> <li>real world aspect : physical phenomena, social organization, humman thought</li> <li>model : set of constants (identifiers), functions (relations), axioms (constraints) <ul> <li>representation of functions : algorithm (implicit), enumaration (explicit) = data</li> <li>interpretation function : homomorphic function (preserve form) from model constant to real-world objects, $I(f(·,\ldots,·))=f_{real}(I(·),\ldots,I(·))$</li> <li>computer system representation : data model that use data structure and operations</li> </ul> </li> <li>definitions : data model $D$, database $DB$, data definition language $DDL$, data manipulation language $DML$, schema $S$</li> <li>abstraction level <ul> <li>knowledge : schema that evolve dynamically, knowledge graph</li> <li>structured data : according to a schema, relational data, rdf, xml</li> <li>unstructured data : measurement data, text, video</li> </ul> </li> <li>data independence : same data interpreted in different ways <ul> <li>logical vs physical realization : use interface</li> <li>pragmatic layer : user/community specific (social utility management), evaluation</li> <li>semantic layer : domain specific (information system), interpretation</li> <li>syntactic layer : domain independent (database system), measurement</li> <li>physical layer : storage (operating system)</li> </ul> </li> </ul> <h3 id="data-management">Data management</h3> <ul> <li>data management tasks : e.g. row vs column store <ul> <li>efficient : storage, indexing, search, aggregation</li> <li>peristence</li> <li>consistency</li> </ul> </li> <li>optimizing access database : physical design (frequencies, predicates, before database access), declarative query optimization (logical, index exploitation, during database access)</li> <li>safe storage and update : concurrency, recovery</li> <li>transaction : isolation (not inter-user influence), atomaticity and durability (failures do not affect)</li> <li>modelling architecture : ANSI <ul> <li>conceptual schema : domain specifc, abstract models</li> <li>logical schema : data models</li> <li>physical schema : physical storage on disk, network</li> </ul> </li> <li>information system modelling <ul> <li>semantic layer : domain specific, information system with interpretation</li> <li>syntactic layer : domain independent, database system</li> <li>physical layer : operation system connecting to database</li> </ul> </li> </ul> <h3 id="information-management">Information management</h3> <ul> <li>information management tasks <ul> <li>retrieval : given a model, find specific data</li> <li>data mining : given data, find a model, higher level abstractions for lower level data</li> <li>conceptual modeling : analyze real world and specify model</li> <li>evaluation : given a model, evalute it against reality</li> <li>control, output : adapt system</li> <li>monitoring, input : collect data</li> </ul> </li> <li>utility of information : value depends on importance and quality of decision</li> </ul> <h3 id="distributed-information-systems">Distributed information systems</h3> <ul> <li>centralization vs distribution : locality, scalability, parallelism <ul> <li>heterogeneous information systems : different models, require interoperable IS</li> <li>autonomy : distribution of control</li> </ul> </li> <li>distributed data management <ul> <li>partitioning</li> <li>replication and caching</li> <li>data access : push, pull, indexing, query distribution</li> <li>communication model <ul> <li>unicast : point-to-point, request-reply</li> <li>multicast : propagate request to mulptile receivers (gossiping)</li> <li>broadcast : wireless channels</li> </ul> </li> <li>dissemination : periodic, conditional, ad-hoc</li> </ul> </li> <li>data overload : usefulnes, starvation when data supply less than data demand</li> <li>semantic heterogeneity : real world aspect modelled differently, relating different model require often humain, caused by automony <ul> <li>standardization : mapping though standards</li> <li>ontologies : mediated mapping, explicit specification of conceptualization of real world</li> <li>mapping : direct mapping, detect correspondance, resolve conflict, integrate schema</li> </ul> </li> <li>syntactic heterogeneity : same data represented using different data models (xml, graphs, rdf)</li> <li>evaluating : trust (reputation-based), quality of information (pagerank), privacy (obfuscation)</li> <li>distributed control : self organized, coordination in large-scale systems needs to be decentralized</li> <li>big data : volume, velocity, variety, veracity</li> </ul> <h2 id="1-information-retrieval">1 Information retrieval</h2> <p><img src="./assets/img/cs423-0.jpg" alt=""/></p> <ul> <li>information retrieval : task of finding in large collection of documents those that satisfy user needs</li> <li>document constituents <ul> <li>social : people, authors, consumers, mentionned, social networks, communities, influence as relationship</li> <li>content : text, media, search, clustering, topics, classification as relationship</li> <li>concept : general ideas, explicit annotation, entities extracted, taxonomies or ontologies as relationship</li> </ul> </li> <li>retrieval model <ul> <li>determines : structure of document, query respresentation and similarity matching function</li> <li>relevance : no objective measure, reflect right topic, user need, authority, recency</li> <li>browser : retrieval ranked result interpreted by system, browsing as interpretation of information by human</li> </ul> </li> <li>evaluation : relevant document R, answer set A, using TREC dataset <ul> <li>unranked result set : all elements are equally important</li> <li>recall : $R=\frac{tp}{tp+fn}=P(retrieved\mid relevant)$</li> <li>precision : $P=\frac{tp}{tp+fp}=P(relevant\mid retrieved)$</li> <li>F-measure : weighted harmonic mean $F=\frac{1}{\alpha\frac{1}{P}+(1-\alpha)\frac{1}{R}}\in[0,1]$</li> <li>F1 : balanced F-measure with $\alpha=\frac{1}{2}$ give $F1=\frac{2PR}{P+R}$</li> <li>arithmetic mean : $\frac{tp+tn}{tp+tn+fp+fn}$, not suitable</li> <li>ranked retrieval : interpolated precision $P_{int}( R)=\max_{R’\ge R} R’$</li> <li> <table> <tbody> <tr> <td>mean average precision : $MAP(Q)=\frac{1}{</td> <td>Q</td> <td>}\sum_{j=1}^{</td> <td>Q</td> <td>}\frac{1}{m_j}\sum_{k=1}^{m_j}P(R_{jk})$ with $Q$ set of queries, with $R_{jk}$ top $k$ documents for query $q_j$</td> </tr> </tbody> </table> </li> <li>specificity : $S=\frac{tn}{fp+tn}=P(notRetrieved\mid notRelevant)$, steeper the better, ROC curve</li> </ul> </li> </ul> <h3 id="text-based-information-retrieval">Text-based information retrieval</h3> <p><img src="./assets/img/cs423-0.1.jpg" alt=""/></p> <ul> <li>text-based information rertieval : <ul> <li>full-text : ignore grammar, meaning, keep only word, layout and metadata can be taken into account</li> <li>pre-processing : tokens, stopwords, stemming, manual indexing</li> <li>inverted file : indexing</li> </ul> </li> <li>basic concepts <ul> <li>document : $d$, express ideas</li> <li>query : $q$, information need</li> <li>index term $k$ : semantic unit (word, short phrase, root of a word)</li> <li>database : $DB$, collection of $n$ documents $d_j\in DB$</li> <li>vocabulary : $T$, collection of $m$ index terms $k_i\in T$</li> <li>index term importance : $d_j=(w_{1j},\ldots,w_{mj})$ with weights $w_{ij}\in[0,1]$</li> <li>similarity coefficient : $sim(q, d_j)$ estimate relevance of $d_j$ for query $q$</li> <li>term-document matrix : contains only terms that occur multiple times, no stop word), term (vertical) vs documents (horizontal)</li> </ul> </li> <li>boolean retrieval : which term should be present <ul> <li>similarity computation <ul> <li>determine disjunctive normal form : $q=ct_1\,OR\,\ldots\,OR\, ct_l$ with $ct=t_1\, AND\,\ldots\, AND\, t_k$ and $t_k\in{t, NOT\, t}$</li> <li>create weight vector $vec(ct)=(w_1,\ldots,w_m)$ for each conjunctive term <ul> <li>$w_i=1$ is $k_i$ occurs in $ct$</li> <li>$w_i=-1$ if not $k_i$ occurs in $ct$</li> <li>$0$ otherwise</li> </ul> </li> <li>if one weight vector of conjunctive term matches document : $d_j$ relevant ($sim(d_j,q)=1$), $vec(ct)$ matches $d_k$ if $w_i=1\wedge w_{ij}=1$ or $w_i=-1\wedge w_{ij}=0$</li> </ul> </li> </ul> </li> </ul> <h3 id="vector-space-retrieval">Vector space retrieval</h3> <ul> <li>vector space retrieval : allow ranking, tolerance with non-binary weights <ul> <li>similarity computation : $sim(q,d_j)=\cos\theta=\frac{d_j\cdot q}{\abs{d_j}\abs{q}}$ <ul> <li>$d_j=(w_{1j},\ldots,w_{mj})$ with $w_{ij}&gt;0$ if $k_i\in d_j$</li> <li>$q=(w_{1q},\ldots,w_{mq})$ with $w_{iq}\ge 0$</li> <li>$\abs{v}=\sqrt{\sum_{i=1}^m v_i^2}$</li> </ul> </li> <li>term frequency : $tf(i,j)=\frac{freq(i,j)}{\max_{k\in T} freq(k,j)}$ of term $k_i$ in $d_j$</li> <li>inverse document frequency : $idf(i)=\log\frac{n}{n_i}$ with $n_i$ number of document in which $k_i$ occurs</li> <li>query weight : $w_{ij}=tf(i,j)idf(i)$ <img src="./assets/img/cs423-1.jpg" alt=""/></li> </ul> </li> </ul> <h3 id="probabilistic-information-retrieval">Probabilistic information retrieval</h3> <ul> <li>probabilistic information retrieval : attempt to directly model relevance as probability <ul> <li>query likelihood model : $P(d\mid q)$ assume $P(d)$ uniform across collection, $P(q)$ same for all documents</li> <li>language modeling : assume each document generated by language model, probabilities for each ngrams</li> <li>query : $P(q\mid M_d)$</li> <li>MLE : $P_{mle}(t\mid M_d)=\frac{tf_{t,d}}{L_d}$ with $tf_{t,d}$ number of occurences of $t$ in $d$ (term frequency) and $L_d$ number of terms in document (document length)</li> <li>model : $P(q\mid M_d)=\Pi_{t\in Q}P_{mle}(t\mid M_d)$</li> <li>smoothed model : $P(t\mid d)=\lambda P_{mle}(t\mid M_d)+(1-\lambda)P_{mle}(t\mid M_c)$ with $M_c$ language model over whole collection, add small weight for non-occuring term $P(t\mid M_d)\le cf_t/T$ for $cf_t$ occurence of $t$ in collection and $T$ total number of terms in collection <ul> <li>model : $P(d\mid q)\propto P(d)\Pi_{t\in q}((1-\lambda)P(t\mid M_c)+\lambda P(t\mid M_d))$, tuning can be query-dependent</li> </ul> </li> </ul> </li> </ul> <h3 id="query-expansion">Query expansion</h3> <ul> <li>query expansion : increase recall by adding terms <ul> <li>local approach : user relevance feedback, use information from current query results <ul> <li>user identifies documet as (non)-relevant</li> <li>collection centroid : $\mu(D)=\frac{1}{\abs{D}}\sum_{d\in D}d$ of document set</li> <li>Rocchio algorithm : find query that optimally separates relevant from non-relevant, $q_{opt}=\arg\max_{q}[sim(q,\mu(D_r))-sim(q,\mu(D_n))]$</li> <li>identifying relevant document : $q_{opt}=\mu(D_r)+[\mu(D_r)-\mu(D_n)]$, but user feedback incomplete, user feedback can be homogeneous</li> <li>smart : practical relevance feedback, $q_{approx}=\alpha q+\frac{\beta}{\abs{D_r}}\sum_{d_j\in D_r}d_j-\frac{\gamma}{\abs{R\setminus D_r}}\sum_{d\not\in D_r}d_j$ with tuning parameters, result $R$ and relevant document $D_r$</li> <li>pseudo-relevance : if no user feedback, choose top-k and apply SMART, works good unless horribly failed (query drift)</li> </ul> </li> <li>global approach : query expansion, use information from a document collection, use query independent resource <ul> <li>automatic thesaurus generation : similary between two words, co-occur, gramatical relation</li> <li>query logs : exploit correlation, users extends query, users refer to same result</li> </ul> </li> </ul> </li> </ul> <h3 id="indexing-for-information-retrieval">Indexing for information retrieval</h3> <ul> <li>indexing for information retrieval <ul> <li>inverted files : word-oriented mechanism, semi-static collection, optimized for search <ul> <li>inverted list : $l_k=&lt;f_k:d_{i_1},\ldots,d_{i_{fk}}&gt;$ with $f_k$ number of document in which $k$ occurs, $d$s document identifiers of document containing $k$</li> <li>lexicographically ordered sequence : $IF=&lt;i,k_i,l_{k_i}&gt;$ <ul> <li>heap’s law : $0.4&lt;\beta&lt;0.6$ <img src="./assets/img/cs423-2.jpg" alt=""/> <img src="./assets/img/cs423-3.jpg" alt=""/> <img src="./assets/img/cs423-4.jpg" alt=""/></li> </ul> </li> </ul> </li> <li>searching <ul> <li>vocabulary search</li> <li>retrieval of occurences</li> <li>manipulation of occurences</li> </ul> </li> <li>construction <ul> <li>search phase : trie storing for each word a list of its occurences, sequentially</li> <li>storage phase : write list of occurences contiguously to disk, write sorted vocabulary, O(n)</li> <li>index : can be merged if not fit in memory $O(n\log_2(n/M))$ with memory $M$, map-reduce</li> </ul> </li> <li>granularity <ul> <li>coarser : text spanning multiple document</li> <li>finer : paragraph, sentence, words</li> <li>general rule : finer granularity the less post-processing but larger index</li> </ul> </li> <li>index compression : replace ordered document identifier in inverted list by their differences (10-15%)</li> </ul> </li> </ul> <h3 id="distributed-retrieval">Distributed retrieval</h3> <ul> <li>distributed retrieval : aggregate weights for all documents <ul> <li>Fagin’s algorithm : enrties in posting lists sorted according to tf-idf weights, scan in parallel all list in round-robin till $k$ documents detected in all list, lookup missing weights for documents not seen in all lists, select top-$k$ elements, $O((k n)^{1/2})$ (assuming non correlated)</li> </ul> </li> </ul> <h3 id="linked-based-ranking">Linked-based ranking</h3> <ul> <li>linked based retrieval <ul> <li>hypertext : anchor text as context, hyperlink as quality signal</li> <li>anchor text : surrounding a hyperlink</li> <li>scoring : weight depdending on the authority of the anchor page</li> <li>nepotistic : promoting your own family members, can give lower weights to links within same site</li> </ul> </li> <li>pagerank <ul> <li>citation analysis <ul> <li>frequency : importance</li> <li>co-citation : related</li> <li>indexing : who is this author cited by</li> <li>authority of sources : impact factor of journals</li> </ul> </li> <li>relevance <ul> <li>number of referrals (incoming links)</li> <li>number of high relevance refferals</li> </ul> </li> <li>scoring <ul> <li>link-based : random equiprobable walk, long-term visit rate is score</li> <li>teleporting : random web page at dead end, probability to jump at each visit</li> <li>model : $P(p_i)=\sum_{p_j\mid p_j\to p_i}\frac{P(p_j)}{C(p_j)}$ for $N$ pages with outgoing links $C(p)$ and probability to visit $P(p_i)$</li> <li> <table> <tbody> <tr> <td>matrix equation : $R_{ij}=\frac{1}{C(p_j)}$ if $p_j\to p_i$ else $0$, $p=(P(p_1),\ldots,P(p_n))$, $p=Rp$ for $</td> <td> </td> <td>p</td> <td> </td> <td>_1=\sum p_i=1$, relevance is highest eigenvalue</td> </tr> </tbody> </table> </li> </ul> </li> <li>pagerank : $P(p_i)=c(\frac{1-q}{N}+q\sum_{p_j\mid p_j\to p_i}\frac{P(p_j)}{C(p_j)})$ for $c\le 1$, $p=c((qR+(1-q)E))p=c(qRp+\frac{(1-q)}{N} e)$ with $E=1/N$ square matrix and $E\cdot p=e$ <img src="./assets/img/cs423-5.jpg" alt=""/></li> </ul> </li> <li>HITS : hyperlink-induced topic search <ul> <li>idea : for a query, find two sets of inter-related pages instead of lists of page <ul> <li>hub page : good lists of links for subject</li> <li>authorative : pages occuring recurrently on good hubs</li> </ul> </li> <li>best : broad topic, post processing</li> <li>computing : 5 iterations to converge in practice <ul> <li>select $N$ pages containing relevant hubs and authorities</li> <li>for each page $p$ compute hub score $H(p_i)=\sum_{p_i\to p_j} A(p_j)$ and authority score $A(p_i)=\sum_{p_j\to p_i} H(p_j)$, normalize sum of square = $1$</li> <li>initialize $h(p)=1/N^2$ and $a(p)=1/N^2$, iteratively update and return high scores</li> </ul> </li> <li>eigenvalue : $y=Lx$ and $x=L^\top y$ give $y^<em>$ eigenvector of $LL^\top$ and $x^</em>$ eigenvector of $L^\top L$, always converge (power iteration) to highest one</li> <li>root set : all pages mentioning the query</li> <li>base set : page that points to root set or is pointed by root set</li> <li>issues : structural anomalies of link structures, tropic drift, mutually reinforcing affiliates</li> </ul> </li> <li>social network analysis : pagerank and hits</li> <li>link indexing <ul> <li>connectivity server : stores mapping in memory from URL to outlinks, URL to inlinks</li> <li>adjaceny lists : set of neighbors of a node, each number represented by an URL, 320 GB for actual web (10 pages per links), but locality (gap encoding, gamma encoding) and similarity is good (copy data from similar lists, reference list, copy lists)</li> </ul> </li> </ul> <h2 id="2-data-mining">2 Data mining</h2> <ul> <li>big data challenge <ul> <li>data : accessible, legal, format</li> <li>questions : searching for insights</li> <li>algorithms : smart and efficient</li> <li>systems : handling such load</li> </ul> </li> <li>local properties <ul> <li>patterns : pattern mining, association rules</li> </ul> </li> <li>global models <ul> <li>descriptive structure of data : clustering, information retrieval</li> <li>predictive function of data : classification, regression</li> <li>exploratory data analysis : interactive tools, visualisation</li> </ul> </li> <li>data mining algorithm <ul> <li>pattern structure/model representation</li> <li>scoring function</li> <li>optimisation and search : tune parameters</li> <li>data management : very large dataset</li> </ul> <h3 id="association-rule-mining">Association rule mining</h3> </li> <li>association rule <ul> <li>form $body\to head\,[support, confidence]$ with $body$ and $head$ being $predicate(x,x\in{items})$</li> <li>multimensional : $body_1\wedge body_2$, can be decomposed to multi single rules</li> <li>support : probability that body and head occur in transaction $p({body, head})$</li> <li>confidence : probability that if body occurs, also head occurs $p({head}\mid{body })$</li> <li>definition <ul> <li>itemset : subset of all items $I$</li> <li>transaction : $(tid, T)$ with $T\subset I$</li> <li>database : $D$ set of all transcactions</li> <li>association rule : $A\to B\,[s,c]$ <ul> <li>$A,B\subseteq I$</li> <li>$A\cap B$ empty</li> <li> <table> <tbody> <tr> <td>$s=p(A\cup B)=count(A\cup B)/</td> <td>D</td> <td>$</td> </tr> </tbody> </table> </li> <li>$c=p(B\mid A)=s(A\cup B)/s(A)$</li> </ul> </li> </ul> </li> <li>problem : find all rules s.t. $s&gt;s_{min}$ (high support) and $c&gt;c_{min}$ (high confidence)</li> <li>two step approach <ul> <li>find frequent itemsets : $J\subseteq I$ s.t. $p(J)&gt;s_\min$, if $A\cup B$, then only $A\to B$ can be an associate rule, any subset of frequent itemset is also frequent (apriori property), find frequent itemsets with increasing cardinality from $1$ to $k$ to reduce search space</li> <li>select pertinent rules : $A\to B$ s.t. $A\cup B=J$ and $P(B\mid A)&gt; c_\min$</li> <li>apriori property : any subset of frequent itemset is also frequent itemset <ul> <li>union of two $k-1$ that differs only by one item is candidate</li> <li>join algorithm : sort itemset in $L_{k-1}$, find all pairs with same first $k-2$ items, but different $k-1$th item, join two itemset,</li> <li>prune : not frequent ones (or not existing) <img src="./assets/img/cs423-5.1.jpg" alt=""/> <img src="./assets/img/cs423-5.2.jpg" alt=""/> <img src="./assets/img/cs423-6.jpg" alt=""/> <img src="./assets/img/cs423-7.jpg" alt=""/></li> </ul> </li> </ul> </li> <li>alternative measures of interest : interesting rule those with high positive or negative interest values <ul> <li>added value : $AV(A\to B)=confidence(A\to B)-support(B)$</li> <li>lift : $lift(A\to B)=\frac{confidence(A\to B)}{support(B)}$</li> </ul> </li> <li>quantitative attributes : transforming quantitative (numeric ordered values) into categorical ones, rules depends on discretisation <ul> <li>static discretisation : predefined bins</li> <li>dynamic discretisation : based on data distributions</li> </ul> </li> <li>apriori for large dataset <ul> <li>transaction reduction : omit non frequent one in subsequent scans</li> <li>sampling : randomly sample with probability $p$, detect frequent itemset with support $ps$, eliminate false positive by counting frequent items on complete data (if sorted, take first ones), false negative can be reduced by choosing lower threshold $0.9ps$</li> <li>partitioning : SON algorithm <ul> <li>divide in $1/p$ partitions, repeatedly read partition into main memory</li> <li>detect in-memory algorithm to find all frequent itemsets with support threshold $ps$</li> <li>itemset becomes candidate if found to be frequent in at least one partition</li> <li>second pass count candidate items and determine which are frequent in entire set</li> <li>monotonicity idea : itemset cannot be frequent in full set without being frequent in at least one partition</li> </ul> </li> </ul> </li> <li>FP growth : frequent itemset discovery without candidate generation <ul> <li>build FP-tree datastructure <ul> <li>compute support for each item : sort items in order of decreasing support (compact tree)</li> <li>construct FP : expand one itemset at a time</li> <li>introduce links between similar labels</li> </ul> </li> <li>extract frequent itemsets directly from FP-tree <ul> <li>bottom-up approach : for each item extract tree with paths ending in the item, start with lowest support</li> <li>conditional : FP-tree that contains selected items, remove nodes of the item, remove nodes with unsufficient support count <img src="./assets/img/cs423-7.1.jpg" alt=""/> <img src="./assets/img/cs423-7.2.jpg" alt=""/></li> </ul> </li> <li>advantages : only 2 passes, compresses dataset, much faster than apriori</li> <li>disadvantages : less efficient with high support threshold, in-memory, difficult to distribute</li> </ul> </li> </ul> </li> </ul> <h3 id="information-retrieval-advanced-models-for-test-representation">Information retrieval advanced models for test representation</h3> <ul> <li>vector spacew retrieval : bad handling of synonym (car and automobile, poor recall) and homony (apple, poor precision)</li> <li>dimensionality reduction : map documents and queries into lower-dimensional space composed of higher-level concepts</li> <li>concept space : normalized concept vector</li> <li>term-document matrix : $M_{ij}$ with $m$ rows (terms) and $n$ columns normalized (documents) given a weight $w_{ij}$ associated with $t_i$ and $d_j$ (can be based on tf-idf weighting scheme) <ul> <li>computing ranking : $M^\top\cdot q$</li> </ul> </li> <li>singular value decomposition : $M=KSD^\top$ with $KK^\top=I=DD^\top$ having orthonormal columns and $S=r\times r$ diagonal matrix of singular values in descreasing order ($r=\min(m,n)$) <ul> <li>$K=eigen(MM^\top)$</li> <li>$D=eigen(M^\top M)$</li> <li>complexity : $O(n^3)$ if $m\leq n$</li> <li> <table> <tbody> <tr> <td>$s_i$ : length of semi-axes of hyperellipsoid $E={Mx \mid</td> <td> </td> <td>x</td> <td> </td> <td>_2=1}$</td> </tr> </tbody> </table> </li> </ul> </li> <li>latent semantic indexing : selection only the largest $s$ from SVD giving $M_s=K_sS_SD_s^\top$, poor statisical explanation <img src="./assets/img/cs423-7.3.jpg" alt=""/></li> <li>cosine similarity : compare columns $(D_s^\top)_i$ and $(D_s^\top)_j$, query $q$ applies same transformation (treated as additional document vector) <ul> <li>mapping $M$ to $D$ : $M=KSD^\top$ as $D=M^\top KS^{-1}$ giving $q^*=q^tK_sS_s^{-1}$</li> <li> <table> <tbody> <tr> <td>similarity : $sim(q^<em>,d_i)=\frac{q^</em>\cdot (D_s^\top)_i}{</td> <td>q^*</td> <td> </td> <td>(D_s^\top)_i</td> <td>}$</td> </tr> </tbody> </table> </li> </ul> </li> <li>probabilistic latent semantic analysis : based on Bayesian Networks</li> <li>latent dirichlet allocation : based on Dirichlet distribution, state-of-the-art, interpretable <ul> <li>idea : assume document collection randomly generated from known set of topics (generative model, for each doc, choose mixture of topics, sample a topic, sample a word), given document collection, reconstruct topic model</li> </ul> </li> <li>word embeddings : neighborhood of word $w$ express a lot about its meaning, model how likely a word and a context occur together <ul> <li>similarity based representation : most successful idea, two words similar when they have similar contexts (syntactic, semantic)</li> <li>context : $C(w)$ giving word context occurence $(w,c),c\in C(w)$</li> <li>model : two matrices $W^{(1)}$ and $W^{(2)}$ to map words and context words <img src="./assets/img/cs423-8.jpg" alt=""/></li> <li>probability : $p(D=1\mid w,c;\theta)=\frac{1}{1+e^{- v_c\cdot v_w}}=\sigma(v_c\cdot v_w)$, whether conext comes from data</li> <li>goal : find $\theta$ s.t. overall probability is maximized $\theta=\arg\max_{\theta}\Pi_{(w,c)\in D}P(D=1\mid w,c,\theta)\Pi_{(w,c)\in\tilde D}P(D=0\mid w,c,\theta)$ (require negative sample $\tilde D$)</li> <li>SGD : $\theta’=\theta + \alpha\Delta_\theta J_t(\theta)$ with $J(\theta)=-\log(\sigma(v_c\dot v_w))-\sum_{k=1}^K\log(\sigma(v_{c_k}\cdot v_w))$ only over row containing word</li> <li>negative sample : obtained from $P_n(w)=V\setminus C(w)$, empirical approach approximate probability by sampling few non-context words or choose probability $p_w^{3/4}$ instead of $p_w$</li> <li>low-dimensional representation : $W=w^{(1)}+W^{(2)}$</li> </ul> </li> <li>CBOW : continuious bag of words models, predict words from context</li> <li>GLOVE : exploits ratio of probabilities to capture semantic relationships among terms</li> </ul> <h3 id="clustering">Clustering</h3> <ul> <li>clustering : assign objects described by attributes to a class</li> <li>model : partition a set of objects into clusters, unsupervised</li> <li>similarity <ul> <li>intra-cluster : high</li> <li>inter-cluster : low</li> </ul> </li> <li>heuristic algorithms : $n$ objects, $k$ clusters, $t$ iteration <ul> <li>K-means : clustered by point whose mean distance minimal, $O(tkn)$, local optimum</li> <li>K-medoids : clustered by object whose mean distance minimal</li> <li>K-medians : clustered by point whose median distance minimal</li> </ul> </li> <li>partioning : given database $D$ of $n$ object, split $D$ into $k$ sets $C_1,\ldots,C_k$ s.t. $C_i\cap C_j=\emptyset$</li> <li> <table> <tbody> <tr> <td>score function : minimize $J=\frac{1}{n}\sum_{i=1}^k\sum_{x_j\in C_i}</td> <td> </td> <td>x_j-\mu_i</td> <td> </td> <td>^2$, $\mu_i=\frac{1}{</td> <td>C_i</td> <td>}\sum_{x_j\in C_i} x_j$</td> </tr> </tbody> </table> </li> <li>categorical attributes : use matching coefficient, distance = #pair-wise mismatches / #features</li> <li>distance function : $d(x,y)\ge 0$, $d(x,y)=0\iff x=y$, $d(x,y)=d(y,x)$, $d(x,z)\le d(x,y)+d(y,z)$</li> <li>density-based clustering : handle noise, cluster in one scan, discover non convex, no need to define cluster number <ul> <li>distance metric : $\epsilon$-neighborhood $N_\epsilon(q)={p\mid d(p,q)&lt;\epsilon }$</li> <li> <table> <tbody> <tr> <td>core point : if $</td> <td>N_\epsilon(q)</td> <td>\ge\mu$</td> </tr> </tbody> </table> </li> <li>directly density-reachable : $p$ from $q$ if $p\in N_\epsilon(q)$ and $|N_\epsilon(q)|\ge\mu$ (not core point but reachable is called border point, otherwise outlier) <img src="./assets/img/cs423-8.1.jpg" alt=""/></li> <li>directed graph : induced</li> <li>density-reachable : $p$ from $q$ if there is chain $p_1,\ldots,p_n$ $p_1=q$ $p_n=p$ s.t. $p_{i+1}$ directly density-reachable from $p_i$ <img src="./assets/img/cs423-8.2.jpg" alt=""/></li> <li>density connected : $p$ to $q$ if there is point $r$ s.t. both $p$ and $q$ density-reachable from $r$, symmetric <img src="./assets/img/cs423-8.3.jpg" alt=""/></li> <li>cluster : set of clusters unique, not necessary disjoint, $C$ satisfies <ul> <li>maximality : if $q$ in $C$ is core point, $p$ also in $C$ if density reachable</li> <li>connectivity : any two points in $C$ must be density connected</li> </ul> </li> <li>DBSCAN <ul> <li>initialization : $V_{core}$ set of core points, $P$ all points, set of clusters $C={}$, $O(n^2)$</li> <li>construction : while $V_{core}$ not empty, $O(n^2)$ <ul> <li>select point $p$ from $V_{core}$ and construct $S(p)$ set of all points density reachable from $p$ (breadth first search on $G$ starting from $p$)</li> <li>$C=C\cup{S(p)}$</li> <li>$P=P\setminus S(p)$</li> <li>$V_{core}=V_{core}\setminus S_{core}(p)$ where $S_{core}(p)$ core points in $S(p)$</li> <li>remaining points : unclustered</li> </ul> </li> </ul> </li> </ul> </li> <li>online incremental clustering</li> </ul> <h3 id="mining-social-graphs">Mining social graphs</h3> <ul> <li>graph : explicit relationships</li> <li>graph cluster : communities, interests, level of trust</li> <li>clique : complete subgraph</li> <li>linked <ul> <li>heavy intra-linked : high intra-cluster similarity</li> <li>scarcely inter-linked : low inter-cluster similarity</li> </ul> </li> <li>hierarchical clustering : iteratively identifies groups of nodes with high similarity <ul> <li>agglomerative algorithm : merge node and communities with high similarity</li> <li>divisive algorithms : split communities by removing links that connect nodes with low similarity</li> </ul> </li> <li>Louvain algorithm : agglomerative <ul> <li>modularity : measure for community quality, higher the better $\sum_{c\;in\; communities}(#edge\;within\;c - expected\;#edges\;within\;c)$ <ul> <li>expected number of edge : unweighted, uniform random graph (null model), total number of edge $m$, $k_i$ degree (outgoing edges of node $i$, $2m$ edge ends), node $j$ has an end with $k_j/2m$</li> <li>measure : $Q=\frac{1}{2m}\sum_{i,j}(A_{i,j}-\frac{k_ik_j}{2m})\delta(c_i,c_j)$ with $A_{ij}$ number of edges between $i$ and $j$, $c_i$ communities of node $i$, $Q\in[-1,1]$, significant $0.3-0.7$</li> </ul> </li> <li>first : find small communities by optimizing modularity locally (maximally) on all nodes, try all possibilities and choose the best</li> <li>then : each small community into a new community, repeat <img src="./assets/img/cs423-8.4.jpg" alt=""/></li> <li>communities : sets of nodes with many mutul connection, much less connections to the outside</li> <li>optimum : modularity determine best level to cutoff hierarchical</li> <li>complexity : $O(n\log n)$</li> </ul> </li> <li>Girvan-newman algorithm : divisive, decomposition by splitting edges with highest separation capacity <img src="./assets/img/cs423-8.5.jpg" alt=""/> <ul> <li>betweenness measure : how well they separate communities, number of shortest paths passing over the edge <ul> <li>BFS : top down, for each node BFS, $#shortest paths(A-X)=\sum_{P\; parent\;of\; X}# shortest paths(A-P)$</li> <li>edge flow : edge flow, $weighttosplit(X)=1+\sum_{c\; child\; of\; X}# edgeweight(X-C)$</li> <li>BFS for each node, determine edge flow for each edge, sum up flow values (divide by 2 edge values as computed in both direction) <img src="./assets/img/cs423-8.6.jpg" alt=""/> <img src="./assets/img/cs423-8.7.jpg" alt=""/></li> </ul> </li> <li>random walk betweenness : pair $n$ and $m$ chosen at random, walker stats at $m$ follow each adjacent link uniformly until $n$, betweenness is probability of crossing $i\to j$ after all possible choices for starting nodes $m$ and $n$</li> <li>until no edges left : remove edges with highest betweenness</li> <li>complexity : one link $O(n^2)$, all links $O(L n^2)$, sparse matrix $O(n^3)$</li> </ul> </li> </ul> <h3 id="classification">Classification</h3> <ul> <li>descriptive modeling : clustering</li> <li>predictive modeling : bayes, k-nearest, random forest, svm, boosted trees, etc.</li> <li>classification : supervised <ul> <li>input : set of objects (database $D$) with categorical/numerical attributes and one class label $C$</li> <li>model : $X^d\to C$</li> <li>output : returns class given the attribut</li> <li>learnt : training set</li> <li>evaluated : test set</li> <li>prediction : unknown labels</li> </ul> </li> <li>characteristics <ul> <li>predictive accuracy</li> <li>speed and scalability : time to build, use the model, in memory, disk</li> <li>robustness : handling noise, outliers, missing value</li> <li>interpretability : understand the model, compactness of the model</li> </ul> </li> <li>decision tree induction : flow chart lie tree <ul> <li>score function : accuracy</li> <li>optimisation : top-down tree construction + pruning</li> <li>construction : examples paritioned recursively based on selected most discriminative attributes</li> <li>stop partitioning : if all samples belong to same class (leaf), no attributes left (majority voting to assign class), no samples left</li> <li>attribute selection entropy : $H(P,N)=-\frac{P}{P+N}\log_2\frac{P}{P+N}-\frac{N}{P+N}\log_2\frac{N}{P+N}$</li> <li>attribute entropy : for attribute $A$ partitioning into $S_1,\ldots, S_v$, $H(A)=\sum_{i=1}^v\frac{P_i+N_i}{P+N}H(P_i,N_i)$, select lowest</li> <li>information gain : $Gain(A)=H(P,N)-H(A)$, select highest</li> <li>pruning : reduce overfitting by stop partitioning when large majority is positive <ul> <li>build full tree and replace nodes with leaves labelled with majority class if classification accuracy does not change</li> <li>minimum description length principle : best model $M_i$ minimize $L(M)+L(D\mid M)$ where $L(M)$ is lenght in bits of the description of the model (#nodes,#leaves,#arcs) and $L(D\mid M)$ is length in bits of description of encoded data with the model (#misclassifications)</li> </ul> </li> <li>extracting classification : if-then rules</li> <li>continous attributes : use binary decision trees <ul> <li>sort data according to attribute value</li> <li>determine value that maximizes the information gain by scanning though the data items</li> <li>scalability : persorting data and maintaining order, for everyt attribute separate and presorted table kept (use hashtable to maintain partition attribution) <ul> <li>selected attribute : partition table into two subtables</li> <li>other attributes : use temporary hash table to associate eac dataitem to its partition, then scanned and partitionned</li> </ul> </li> </ul> </li> <li>characteristics : automatic feature selection, minimal data preparation, non-linear model, easy to interpret, senstive to perturbation, overfit</li> </ul> </li> <li>ensemble methods : collection of simple/weak learner, combine results to make strong learner <ul> <li>bagging : train learner parallel on different samples, combine by voting or averaging</li> <li>stacking : combine models using a second stage learner like regression</li> <li>boosting : train learner on filtered output of other learners</li> </ul> </li> <li>random forest : learn $K$ different decision trees from independent samples, vote between different learners so model not too similar <ul> <li>aggregate output : majority vote</li> <li>sampling strategies : subset of different data, subset of attributes</li> <li>algorithm : typical parameters $m\approx\sqrt(p)$, $K\approx 500$ for $p$ total attributes <ul> <li>draw $K$ bootstrap samples of size $N$ from original dataset with replacement (bootstrapping)</li> <li>contruct decision tree and select a random set of $m$ attributes out of $p$ to infer split (feature bagging)</li> </ul> </li> <li>characteristics : complex decision without overfitting, popular for dense data (thousand of features), easy to implement, good for map reduce, worse than deep neural net, need many pass, hard to balance</li> </ul> </li> </ul> <h3 id="classification-methodology">Classification methodology</h3> <ul> <li>credibility : trustworthiness (well-intentioned, truthful, unbiased) + expertise (knownledge, experienced, competent)</li> <li>data collection and preparation <ul> <li>feature identitifcation : domain knownledge is needed <ul> <li>numerical : maybe discretisation</li> <li>ordinal</li> <li>categorical</li> </ul> </li> <li>labelling : time consuming, expensive, ask expert, ask crowd, complementary source of information (distant learning) <ul> <li>crowd-workers <ul> <li>truthful : expert, normal</li> <li>untruthful : sloppy (limited knownledge, misunderstanging), uniform spammer, random spammer</li> </ul> </li> <li>non-iterative aggregation algorithms : process answers matrix and produce estimate of most likely answer to be correct</li> <li>majority decision : MD, no preprocessing, $P(x_j=l)=\frac{1}{N}\sum_i^N(1\mid a_i(x_j)=l)$ with $x_j$ object to label $l$,</li> <li>honey pot : HP, insert known labels, remove workers that fail at correctly labelling more that $m$%, then majority decision</li> <li>iterative aggregation algorithms : estimate worker expertise from answers</li> <li>expectation maximisation : EM <ul> <li>e-step : estimate label from answers of workers, $P(x_j=l)=\frac{1}{\sum_{i=1}^N w_i}\sum_{i=1}^N (w_i\mid a_i(x_j)=l)$</li> <li>m-step : estimate reliability of workers from consistency of answers, expertise $w_i=\frac{1}{M}\sum_{j=1}^M(1\mid a_i(x_j)=\arg\max P_l(x_j=l))$</li> </ul> </li> </ul> </li> <li>discretization <ul> <li>unsupervised discretisation : equal with, equal frequency, clustering</li> <li>supervised discretisation : independence test $x^2$ statistics, $x^2=\sum_i\sum_j \frac{(O_{ij} - E_{ij})^2}{E_{ij}}$ with $O_{ij}$ observed frequency and $E_{ij}$ expected frequency, independent if $P(x^2\mid DF=1)&gt;0.05$ with $DF$ degree of freedom ($=(#row - 1)(#cols -1)$), merge interval</li> </ul> </li> <li>feature selection : ${N\choose M}$ subsets, select $M$ optimal features, correlation is not causality, collective relevant features may look individually irrelevant <ul> <li>filtering : consider feature as independent, rank them according to predictive power ($P(x^2\mid DF=n-1)$ give rank measure) <ul> <li>mutual information : numerical $I(F;C)=H( C)-H(C\mid F)=H(F)+H( C)-H(F,C)$</li> </ul> </li> <li>wrapper : consider dependencies among features, create classifier at each iteration and evaluate its performance, stop if no improvement</li> </ul> </li> <li>feature normalization : classifier are sensitive to scale <ul> <li>standardization : $x_i’=\frac{x_i-\mu_i}{\sigma_i}$ normal distribution</li> <li>scaling : $x_i’=\frac{x_i-m_i}{M_i-m_i}$ map to interval $[0,1]$, bad with outliers</li> </ul> </li> </ul> </li> <li>model training, selection and assessment <ul> <li>selecting performance metrics <img src="./assets/img/cs423-8.8.jpg" alt=""/> <ul> <li>accuracy : $A=\frac{TP+TN}{TP+TN+FP+FN}=\frac{TP+TN}{N}$, when classes not skewed, error same importance</li> <li>precision : $P=\frac{TP}{TP+FP}$</li> <li>recall : $R=\frac{TP}{TP+FN}$</li> <li>F-score (F1) : $F1=2\frac{PR}{P+R}$</li> <li>F-beta score : $F_\beta=\frac{(1+\beta^2)PR}{\beta^2P+R}$</li> </ul> </li> <li>model selection : tune parameters (regularisation factor, threshold, distance function, number of neighbours) <ul> <li>train, test (model assessment), validation (model selection)</li> <li>loss function <ul> <li>categorical output : $J=\sum_{i=1}^n #(y\not = f(x_i))$</li> <li>real value output : $J=\frac{1}{n}\sum_{i=1}^n(y_i-f(x_i))^2$, absolute</li> </ul> </li> </ul> </li> <li>organizing training and test data <ul> <li>k-fold cross validation : $K-1$ training set, $1$ validation, avergage of $K$</li> <li>leave one out cross validation : at extreme k-fold, $N-1$ training set, $1$ validation, avergage of $N$</li> <li>fight skew <ul> <li>stratification : validation set as random sample but ensure approximately proportionally represented</li> <li>over and under sampling : including over proportionally number from smaller class (over-sampling) and under proportional number from larger class (under-sampling)</li> </ul> </li> <li> <table> <tbody> <tr> <td>good model : good estimate $f:X^d\to Y$ with error $err(f_d,T)=\frac{1}{</td> <td>T</td> <td>}\sum_{X\in T}(f_D(X)-y)^2$ with $T$ validation set</td> </tr> </tbody> </table> </li> <li>expected error : $EErr_{train}=E_D[Err(f_D,D)]$</li> <li>test error : $EErr_{test}=E_{D,T}[Err(f_D,T(D))]=bias^2+variance$ <ul> <li>bias : $bias=E_{D,T}[f_D(X)-y]$, high is a sign of under-fitting (simple model)</li> <li>variance : $variance =E_{D,T}[(f_D(X)-E_D[f_D(X)]^2]$, high is a sign of over-fitting (complex model)</li> </ul> </li> </ul> </li> </ul> </li> </ul> <h3 id="document-classification">Document classification</h3> <ul> <li>document classification : unstructured documents, spam filtering, sentiment analysis, document filtering, features <ul> <li>words of documents : bag of words, document vector</li> <li>detailed information on words : phrases, word fragments, grammatical features</li> <li>metadata about document and its author</li> <li>challenge : very high dimension, need feature selection (mutual information), dimensionality reduction (word embedding), scalable algorithms</li> </ul> </li> <li>k-Nearest-Neighbors : vector space model <ul> <li>retrieve k nearest neighbors</li> <li>choose majority class label : estimate $P(C\mid D)\approx #C/k$ probablity document has indeed class $C$</li> <li>small $k$ : (simple model), low bias, high variance</li> </ul> </li> <li>naive Bayes classifier : probablistic retrieval, bag of words <ul> <li> <table> <tbody> <tr> <td>how characteristic $w$ for $C$ : $P(w\mid C)=\frac{</td> <td>w\in D,D\in C</td> <td>+1}{\sum_{w’}</td> <td>w’\in D,D\in C</td> <td>+ 1}$</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>how frequent $C$ : $P( C)=\frac{</td> <td>D\in C</td> <td>}{</td> <td>D</td> <td>}$</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>Bayes law : $P(C\mid D)\propto P( C)\Pi_{w\in D}P(w</td> <td>C)$</td> </tr> </tbody> </table> </li> <li>most probable class : $C_{NB}=\arg\max_C(\log P(C )+\sum_{w\in D}\log P(w\mid C))$</li> </ul> </li> <li>word embeddings : <ul> <li>probability $w$ occurs with context word $c$ : $P(D=1\mid w,c;\theta)$</li> <li>consider instead of (word, context) : (class, paragraph)</li> <li>learn : $P(C\mid p)=\frac{e^{v_p\cdot v_c}}{\sum_{C’}e^{-v_p\cdot v_{C’}}}$</li> <li>Fasttext: classifier based on word embedding, n-grams (phrases), subword information (character n-grams)</li> </ul> </li> </ul> <h3 id="recommender-systems">Recommender systems</h3> <ul> <li>model : users, items ranks items in order of descreased relevance</li> <li>collaborative-based : tell me what other people like <ul> <li>wisdom of the crowd : users give ratings to items, user with similar tastes in the past will have similar tastes in the future</li> <li>users based : estimate rating $r_U(I)$ : find set of users $N_U$ who liked the same items as $U$ and rated $I$, aggregate ratings of $I$ provided by $N_U$, cold start, not scalable, mean rating $r_x$ of user $x$ <ul> <li>Pearson correlation coefficient : $sim(x,y)=\frac{\sum_{i=1}^N(r_x(i)-\bar r_x)(r_y(i)-\bar r_y)}{\sqrt{\sum_{i=1}^N(r_x(i)-\bar r_x)^2}\sqrt{\sum_{i=1}^N(r_y(i)-\bar r_y)^2}}$ between $-1$ and $1$</li> <li>cosine similarity : $sim(x,y)=\cos(\vartheta)=\frac{\sum_{i=1}^N r_x(i)r_y(i)}{\sqrt{\sum_{i=1}^N r_x(i)^2}\sqrt{\sum_{i=1}^N r_y(i)^2}}$</li> <li> <table> <tbody> <tr> <td>common aggregation : $r_x(a)=\bar r_x+\frac{\sum_{y\in N_U(x)} sim(x,y)(r_y(a)-\bar r_y)}{\sum_{y\in N_U(x)}</td> <td>sim(x,y)</td> <td>}$</td> </tr> </tbody> </table> </li> </ul> </li> <li>item-based : replace users by items $N_I(a)$, more stable, can be computed in advance, runtime neighboorhood small $N_I(a)$, item $b$ rated by $x$ <ul> <li> <table> <tbody> <tr> <td>common aggregation : $r_x(a)=\frac{\sum_{b\in N_I(a)}sum(a,b)r_x(b)}{\sum_{b\in N_I(a)}</td> <td>sim(a,b)</td> <td>}$</td> </tr> </tbody> </table> </li> </ul> </li> </ul> </li> <li>content-based : show me more of what I liked <ul> <li>find items similar to past items, aggregate ratings of most similar, cold start, recommend more of the same</li> <li>TF-IDF description weight : $w(t,a)=tf(t,a)idf(t)=\frac{freq(t,a)}{\max_{s\in T} freq(s,a)}\log\frac{N}{n(t)}$ with $n(t)$ number of items where term $t$ appears, after pre-processing (stopmwords, stemming, top M terms) <ul> <li>cosine similarity : $sim(a,b)=\cos(\vartheta)=\frac{\sum_{t=1}^T w(t,a)w(t,b)}{\sqrt{\sum_{t=1}^T w(t,a)^2}\sqrt{\sum_{t=1}^T w(t,b)^2}}$ with items $a$, $b$, term $t$</li> <li> <table> <tbody> <tr> <td>aggregation : $r_x(a)=\frac{\sum_{b\in N_I(a)} sim(a,b)r_x(b)}{\sum_{b\in N_I(a)}</td> <td>sim(a,b)</td> <td>}$</td> </tr> </tbody> </table> </li> </ul> </li> </ul> </li> <li> <table> <tbody> <tr> <td>matrix factorization : $\min_{q,p}\sum_{(u,i)\in M}(r_u(i)-q_i^\top p_u)^2+\lambda(</td> <td> </td> <td>q_i</td> <td> </td> <td>^2+</td> <td> </td> <td>p_u</td> <td> </td> <td>^2)$, incomplete matrix, SGD</td> </tr> </tbody> </table> </li> </ul> <h2 id="3-knowledge-modeling">3 Knowledge modeling</h2> <ul> <li>implicit knowledge : information retrieval, data mining</li> <li>explicit knowledge : knowledge modelling</li> </ul> <h3 id="semi-structured-data">Semi-structured data</h3> <ul> <li>schemas : datastructure for databases, relationa, xml, agreement on data structures, consistency, integrity, optimize query</li> <li>HTML : too limited, no schema, no constraints, hard to analyze</li> <li>semi-structured data : contains tags, markup to specify semantics, and relate values (e.g. hierarchically), embeds schema into data, email, json, xml <ul> <li>application-specific markup : making meaning of data explicit (through tags), XML extensible markup language</li> <li>serialization : canonical encoding into a text</li> <li>schema-less data : flexible, self-contained, but consistency and optimization not feasible <img src="./assets/img/cs423-8.9.jpg" alt=""/></li> </ul> </li> <li>document mode : serialized representation</li> </ul> <h3 id="semantic-web">Semantic web</h3> <ul> <li>semantic web : extension of current web in which information is given well-defined meaning <ul> <li>overcome semantic heterogeneity <ul> <li>standardization (integrated approach) : common user-defined, power play enforces it, integrated approach (exists common format, detailled, aggred upon by all parties)</li> <li>translation (federated approach) : mappings, require human, difficult, federated approach (no common format, accommodate on the fly, no party imposes models, languages and method of work)</li> <li>annotation (unified approach) : create relationships to agreed upon conceptualizations, meta-model, IS-A, unified approach (common format but only meta-level)</li> </ul> </li> <li>ontologies : explicit specification of a conceptualization of the real world, proxy representation (annotation) <ul> <li>ontology engineering : manual effort, edit and check</li> <li>automatic induction : from large document collections</li> <li>modeling/encoding : what does an arrow/instance-of/ISA mean? use tag attribute to reference the concept <img src="./assets/img/cs423-9.jpg" alt=""/></li> </ul> </li> </ul> </li> </ul> <h3 id="rdf-resource-description-framework">RDF resource description framework</h3> <ul> <li>resource decription framework : RDF, graph oriented data model to annotate any kind of XML document, similar to ER model <img src="./assets/img/cs423-9.0.jpg" alt=""/> <ul> <li>statements : about resources (URI-addressable) and literals (XML data)</li> <li>resource form : subject (URI) property object (URI or string)</li> <li>RDF statement : also resources</li> <li>properties : define relationship to other resources or atomic values</li> <li>schema : grammar and vocabulary for semantic domains</li> <li>containers : bag (unordered multi-set), seq (ordered), alt (alternatives, single resource chosen out of given set) <img src="./assets/img/cs423-9.01.jpg" alt=""/></li> <li>typing resources : special property rdf:type</li> <li>new resources : special property rdf:ID</li> <li>quantifiers : about, aboutEach</li> <li>reification : introduce resources which serves as representative for a statement <img src="./assets/img/cs423-9.02.jpg" alt=""/></li> </ul> </li> <li>RDF schema <ul> <li>categorization : into classes</li> <li>constraints : possible uses of properties (connect resources) <ul> <li>domaines : classes of which instances may have a property</li> <li>range : classes of which the instances may be the value of a property <img src="./assets/img/cs423-9.1.jpg" alt=""/> <img src="./assets/img/cs423-9.2.jpg" alt=""/></li> </ul> </li> <li>inheritance <ul> <li>A subClassOf B : transitive, not reflexive, anti-symmetric (no cycle), many subclass and superclass, subclass inherit all properties of superclass</li> <li>P1 subPropertyOf P2 : if A has property P1 with value B then it has value B with property P2</li> </ul> </li> </ul> </li> <li>classification <img src="./assets/img/cs423-10.jpg" alt=""/></li> </ul> <h3 id="semantic-web-resources">Semantic web resources</h3> <ul> <li>sematic web resources : wikidata, google knowledge graph, lined open data example, multi-encoding (json, rdf) <ul> <li>wordnet : synonymy/antonymy, herpnymy (hierarchical relationship between words)/hyponymy, meronymy (part-whole relationship, e.g. paper and book)</li> <li>schema.org : big companies behind</li> </ul> </li> </ul> <h3 id="onotology-languages">Onotology languages</h3> <ul> <li>ontology languages : OWL, well designed, well defined, compatible with RDF, rich compared to RDF <ul> <li>description logics (DL) : fragment of first order predicate logic (FOL), reasoning can be done, describe knownledge in term of concepts and role restrictions used to automatically derive classification taxonomies <img src="./assets/img/cs423-11.jpg" alt=""/></li> <li>frame-based systems : central modeling primitive classes with attributes</li> <li>web standards : XML, RDF</li> <li>class constructor : intersectionOf, unionOf, complementOf, oneOf, allValuesFrom, someValuesFrom, maxCardinality, minCardinality <img src="./assets/img/cs423-12.jpg" alt=""/></li> <li>OWL axioms <img src="./assets/img/cs423-13.jpg" alt=""/></li> <li>OWL lite : subset of OWL, easy to omplement</li> <li>OWL DL : restriction to FOL fragment, mixing of RDFS and OWL restricted, disjointness of classes, properties, individuals and data values</li> <li>OWL Full : union of OWL syntax and RDF, no restriction</li> </ul> </li> </ul> <h3 id="information-extraction">Information extraction</h3> <ul> <li>populating knowledge bases automatically : extract knowledge from documents, challenge natural language <ul> <li>concept : ideas or concrete entities, relationship</li> <li>identifying : explicit (twitter tag, keywords) or extract</li> </ul> </li> <li>key phrase extraction : automatic selection of important and topical phrases fro document body <ul> <li>candidate phrases : removing stop word, use word n-grams, part-of-speech</li> <li>baseline ranking approach : ranking according to tf-idf</li> <li>advanced approach : use many structural, syntactic document features, external resources</li> </ul> </li> <li>named entity recognition : find and classify names of people, organizations, places, brands mentionned in documents <ul> <li>uses : sentiment attributed to produtcts, anchors</li> <li>sequence labelling task : classification, predict next label, naive bayes, HMM, MEEM, CRF <ul> <li>features : neighboring words, preceding labels, POS, prefix, suffix, wird shape</li> </ul> </li> <li>generative probabilistic model : words (known) $W=(w_1,\ldots,w_n)$, states (unknown) $E=(e_1,\ldots,e_n)$ <ul> <li>assume text produced : $P(E,W)$</li> <li>model : $\arg\max_E P(E\mid W)=\arg\max_E P(E)P(W\mid E)$</li> <li>transition probabilities : bigram model, $P(E)\approx \Pi_{i=2,\ldots, n} P_E(e_i\mid e_{i-1})$, $P_E(e_i\mid e_{i-1})$ estimated by maximum likelihood</li> <li>word emission probailities : $P(W\mid E)\approx\Pi_{i=1,\ldots,n}P_W(w_i\mid e_i)$, $P_W(w_i\mid e_i)$</li> </ul> </li> <li>smoothing : unseen words might only miss in training set, $P_{WS}(w_i\mid e_i)=\lambda P_W(w_i\mid e_i)+(1-\lambda)\frac{1}{n}$</li> </ul> </li> <li>information extraction : extract structured information from text <ul> <li>statement : IS-A, RELATED-TO, DEPENDS-ON, LOCATED-IN</li> <li> <p>typed statements : between types, PART-OF, RELATED-TO, LOCATED-IN</p> </li> <li> <table> <tbody> <tr> <td>hand-written pattern : “Y such as X ((, X)* (, and</td> <td>or) X)”, NER then “cures(DRUG, DISEASE)”, high-precision, tailored to specific domains, human low recall</td> </tr> </tbody> </table> </li> <li>supervised machine learning <ul> <li>training set : hard to create, choose relevant NER and relations, use unlabeled entity pairs as negative samples</li> <li>classifier : naives bayes, filtering classifier (detect whether relation exists among entities), relation-specific classifier (detect relation label)</li> <li>features : bag of words, bigrams, headword, stems, types, syntactic features, parse tree</li> </ul> </li> <li>bootstrapping : no training data, few high-precision patterns, find entity pairs that match pattern, find sentences containing those, generalize entities and generate new patterns</li> <li>semantic drift : LOC hostels, LOC but geneva not located in lausanne</li> <li>confidence : confirmed set of pairs of mentions $M$, $conf(p)=\frac{hits_p}{finds_p}\log(finds_p)$ <ul> <li>$hits_p$ : set of tuples in $M$ that new pattern matches</li> <li>$finds_p$ : total set of tuples that new pattern matches</li> </ul> </li> <li>distant supervision : no training data, use large corpus to collect training data, classify to predict label from other sources, high precision but low recall, feasible, match only if all individual features match</li> </ul> </li> </ul> <h3 id="taxonomy-induction">Taxonomy induction</h3> <ul> <li>taxonomy induction : extract related facts from documents (classification of animals), not unique <ul> <li>hyponyms : subordinate terms, can inherit properties from hypernyms (more general terms)</li> <li>ISA : no need to learn inferred facts as transitive</li> <li>task : start from root/basic concept <ul> <li>learn relevant terms : hypernym/hyponym relationship</li> <li>filter out : erroneous terms and relation</li> <li>induce : taxonomy structure<br/> <img src="./assets/img/cs423-14.jpg" alt=""/></li> </ul> </li> <li>learning hypernym <img src="./assets/img/cs423-15.jpg" alt=""/></li> <li>inducing hypernym graph : many possible relationships among concepts and terms not likely to be discovered <img src="./assets/img/cs423-16.jpg" alt=""/></li> <li>cleaning hypernym graph : determine all basic concept (not hypernym of another concept), determine all root concepts (no hypernyms), for each basic root concept pair, select all hypernym paths that connect them, choose longest one for final taxonomy</li> </ul> </li> </ul> <h3 id="schema-integration">Schema integration</h3> <ul> <li>keys task in distributed information management : more data ? more models ? more useful information ? supply vs demand ?, interpretation and different views of data source</li> <li>schema matching : integration of heterogeneous data sources <img src="./assets/img/cs423-16.1.jpg" alt=""/> <ul> <li>manual : common practice today</li> <li>tools : based on structural and content features, estiablish correspondences and rank according to quality (error frequent and unavoidable)</li> <li>universe of database : universe $U$ finite set of possible instances</li> <li>similarity of classes : $A$, $B$ classes (subsets of $U$), jaccard $sim(A,B)=\frac{\abs{A\cap B}}{\abs{A\cup B}}=\frac{P(A,B)}{P(A,B)+P(\bar A,B)+P(A,\bar B)}$, does not work when information has structurally different representation <img src="./assets/img/cs423-16.2.jpg" alt=""/></li> <li>classes : intension is similar (instance), extension is different, complex features <ul> <li>attributes and relationship names</li> <li>structural relationship : data types</li> <li>distribution features : data values</li> <li>content features : text <img src="./assets/img/cs423-17.jpg" alt=""/></li> </ul> </li> <li>finding corresponding classes : $U1$ (DB1) and $U2$ (DB2), $U1\cap U2=\emptyset$ <ul> <li>probabilistic approach : give instance $i$ with feature $T_i$ belong to class $A$ <ul> <li>naives bayes : $P(A\mid T_i)=P(T_i\mid A)P(A)P(d)\propto P(T_i\mid A)P(A)$</li> <li>known : $P(A)=\frac{\abs{A}}{\abs{U_1}}$</li> <li>independence assumption $P(T_i\mid A)=P(t_1\mid A)\cdots P(t_n\mid A)$</li> <li>$T_A$ being bag of all terms in all instances of $A$ : $P(t\mid A)=\frac{\abs{t\in T_A}}{\sum_{t’}\abs{t’\in T_A}}$ <img src="./assets/img/cs423-18.jpg" alt=""/> <img src="./assets/img/cs423-19.jpg" alt=""/></li> </ul> </li> </ul> </li> </ul> </li> <li>node mapping : alternative class mapping <ul> <li>naive approach : order matchings by probability, choose most probable matching, produce mapping among the classes, remove mapped classes, choose next most probable matching and repeat, consistency constraints may be violated</li> </ul> </li> </ul> <h3 id="networked-schema-integration">Networked schema integration</h3> <ul> <li>data integration networks : different experts may contribute input on schema matching</li> <li>wisdom of network : leverage knowledge from network <ul> <li>pay-as-you-go schema matching : generate probabilistic matching network (pSMN), keep all potential mappings and assess probability of correctness <img src="./assets/img/cs423-19.1.jpg" alt=""/></li> <li>matching instances : consistency constraints satisfied, drop as little knowledge as possible <ul> <li>uniqueness : attribute cannot be mapped to two different attributes</li> <li>transitivity</li> <li>maximal : minimal number of claimed correspondences removed</li> <li>probability of correctness $p_c$ with correspondence $c$ : $p_c=\frac{\text{#number of matching instances that contain c}}{\text{#total number of matching instances}}$, high complexity computation, use non-uniform sampling for efficient approximation <img src="./assets/img/cs423-19.2.jpg" alt=""/></li> </ul> </li> <li>reducing network uncertainty : $H( C)=-\sum_{k\in C}p_c\log p_c+(1-p_c)\log(1-p_c)$ with information gain $IG( C)=H( C)-H(C\mid c)$</li> <li>selecting best matching instance : after obtaining user feedback, find matching instance with minimal repair distance (dropping as few correspondence as possible), maximal likelihood ($\Pi_{c\in C}p_c$) <img src="./assets/img/cs423-20.jpg" alt=""/></li> </ul> </li> </ul>]]></content><author><name>TC YU</name></author><category term="Courses"/><category term="machine learning"/><summary type="html"><![CDATA[\[\def\abs#1{\left|\,#1\,\right|}\]]]></summary></entry><entry><title type="html">DIS - Classification</title><link href="https://lialittis.github.io/blog/2020/DIS-Classification/" rel="alternate" type="text/html" title="DIS - Classification"/><published>2020-06-11T00:00:00+00:00</published><updated>2020-06-11T00:00:00+00:00</updated><id>https://lialittis.github.io/blog/2020/DIS%20-%20Classification</id><content type="html" xml:base="https://lialittis.github.io/blog/2020/DIS-Classification/"><![CDATA[<head> <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script> </head> <h1 id="dis---classification">DIS - Classification</h1> <h2 id="clustering-and-classificatioin">Clustering and Classificatioin</h2> <p>For inferring global models of data collections there exist two types of approaches: descriptive and predictive modeling. We illustrate the difference among them by an example.</p> <p>We assume that a set of data items (or objects) with two attributes a1 and a2 is given. Assume the global model we are interested in is a classification (or as often said labeling) of the data items. In descriptive modeling we just know the data items, as indicated by the points in the 2-dimensional grid. A descriptive modeling technique, such as clustering, produces classes, which are not known in advance. For doing this it relies on some criteria that specify when two data items probably belong to the same class. Such a criteria is usually based on a similarity measure.</p> <p><img src="https://raw.githubusercontent.com/lialittis/lialittis.github.io/master/assets/img/DIS_Clustering.png" alt=""/></p> <p>A predictive modeling technique, such as classification, starts from a given classification (or labeling) of data items. Using that classification of the dataset the classification method infers conditions on the properties of the data objects, that allow to predict the membership to a specific class. For example, the prediction could be based on a partitioning of the attribute values along each dimension, as shown in the figure on the right. There, first attribute a1 is partitioned into two intervals, and for each of the intervals a different partitioning of the attribute a2 is used to determine the regions corresponding to classes. Misclassifications may occur as seen in the example.</p> <p><img src="https://raw.githubusercontent.com/lialittis/lialittis.github.io/master/assets/img/DIS_Classification.png" alt=""/></p> <ul> <li><strong>Given a dataset of objects described by attributes, build a model that assigns objects to a class (or label)</strong></li> </ul> <p>no class info -&gt; descriptive model(clustering) -&gt; describe classes based on similarity of attribute values</p> <ul> <li><strong>Given a dataset of objects described by attributes, build a model that assigns objects to a class</strong></li> </ul> <p>class info -&gt; predictive model(classification) -&gt; predict classes based on known attribute values</p> <h2 id="classification-problem">Classification Problem</h2> <p>Classification creates a <strong>global model</strong>, that is used for predicting the class label of unknown data. Since the classification function is learned from existing data, this approach is also called a <strong>supervised learning approach</strong>.</p> <p>Classification is clearly useful in many decision problems, where for a given data item a decision is to be made (which depends on the class to which the data item belongs). Classification is also often called <strong>predictive analytics</strong>.</p> <p>Input: set of objects with <strong>categorical/numerical attributes</strong> and one <strong>class label</strong></p> <p>Output: A model that returns the class label given the object attributes</p> <ul> <li>Model is a function represented as rules, decision trees, formulae, neural networks</li> </ul> <p><strong>Classification belongs to supervised ML</strong></p> <ul> <li>Objects have class information</li> </ul> <h2 id="basic-approach">Basic Approach</h2> <p>Model is learnt from a set of objects with known labels: <strong>training set</strong></p> <p>The quality of the model is evaluated by comparing the predicted class labels with those from a set of objects with known labels: <strong>test set</strong></p> <ul> <li>Test set is independent of training set, otherwise over-fitting will occur</li> </ul> <p>The model is applied to data with unknown labels: <strong>prediction</strong></p> <h3 id="example-of-classification">Example of Classification</h3> <p><img src="https://raw.githubusercontent.com/lialittis/lialittis.github.io/master/assets/img/example_classification.png" alt=""/></p> <h4 id="model-test-and-usage">Model test and Usage</h4> <p><img src="https://raw.githubusercontent.com/lialittis/lialittis.github.io/master/assets/img/model_test.png" alt=""/></p> <h2 id="problem-formulation">Problem formulation</h2> <p>Problem: Given a database D with n data items described by d categorical/numerical attributes and one categorical attribute (class label C)</p> <p>Find:(rules, decision tree,…)</p> <p>A function f: $X^d -&gt; C$</p> <p>Such that: classifies accurately the items in the training set generalises well for the (unknown) items in the test set</p> <h2 id="characteristics-of-classification-methods">Characteristics of Classification Methods</h2> <ul> <li>Predictive accuracy</li> <li>Speed and scalability <ul> <li>Time to build the model(usually more expensive)</li> <li>Time to use the model</li> <li>In memory vs. on disk proceesing</li> </ul> </li> <li>Robustness <ul> <li>Handling noise, outliers and missing values</li> </ul> </li> <li>Interpretability [可解释性] <ul> <li>Understanding the model and its decisions(black box) vs. white box</li> <li>Compactness of the model [简洁性]</li> </ul> </li> </ul> <p>As for clustering methods, we can also identify for classification methods a range of criteria to assess and compare the properties of different approaches.</p> <p>Predictive accuracy is the natural main objective to optimize for a classifier. It characterizes how well the classifier performs its job. Often we encounter a trade off between predictive accuracy and the speed and scalability of the method. Methods that achieve high accuracy tend also to be more expensive. As for clustering, also for classification noise and outliers can pose additional problems affecting accuracy. Finally, a very important criterion is the interpretability of the model. In many concrete applications, it is critical that humans are able to understand based on which criteria a classifier takes a decision, e.g. for accountability. Imagine a case, where an assurance policy is refused based on the decision taken by a classifier, and the client would oppose in court. Only with human-interpretable methods the decision could be argued for. However, the most powerful classifiers today tend to produce models that are very hard to interpret for humans, as they represent very complex functions.</p> <h2 id="qa">Q&amp;A</h2> <p>If a classifier has 75% accuracy, it means that …</p> <p>A. It correctly classifies 75% of the data items in the training set</p> <p>B. It correctly classifies 100% of the data items in the training set but only 75% in the test set</p> <p>C. It correctly classifies 75% of the data items in the test set</p> <p>D. It correctly classifies 75% of the unknown data items</p> <p>Answer: [&lt;font color: white&gt; C &lt;/font&gt;]</p> <h2 id="decision-trees">Decision Trees</h2> <ul> <li><strong>Nodes</strong> are tests on a single attribute</li> <li><strong>Branches</strong> are attribute values</li> <li><strong>Leaves</strong> are marked with class labels</li> </ul> <p><img src="https://raw.githubusercontent.com/lialittis/lialittis.github.io/master/assets/img/DIS_Decision_Tree.png" alt=""/></p> <p>A standard type of classification function is a decision tree. In a basic decision tree, <strong>at each level one of the available attributes is used to partition the data set based on the different attribute values</strong>. At the leaf level of the decision tree, the values of the class label attribute are found. Thus, for a given data item with unknown class label attribute, by traversing the tree from the root to the leaf according to its data values, its class can be determined by choosing the class label found at the leaf level. Note that in different branches of the tree, different attributes may be used for classification.</p> <p>A decision tree is constructed in a top-down manner, by recursively splitting the training set using conditions on the attributes. How these conditions are determined is one of the key questions for decision tree induction. After the decision tree construction, it may occur that at the leaf level the granularity is too fine, i.e., many leaves correspond to outliers in the data. Thus, in a second phase such leaves are identified and eliminated.</p> <p>The key problem in constructing a decision tree is thus to determine the attributes that are used to partition the data set at each level of the decision tree.</p> <h3 id="decision-tree-induction-algorithm">Decision Tree Induction: Algorithm</h3> <p><strong>Tree construction (top-down divide-and-conquer strategy)</strong></p> <ul> <li>At the beginning, all training samples belong to the root</li> <li>Examples are partitioned recursively based on a selected “most discriminative” attribute</li> <li>Discriminative power determined based on <strong>information gain (ID3/C4.5)</strong></li> </ul> <p><strong>Partitioning stops if</strong></p> <ul> <li>All samples belong to the same class → assign the class label to the leaf</li> <li>There are no attributes left → majority voting to assign the class label to the leaf</li> <li>There are no samples left</li> </ul> <p>The basic algorithm for decision tree induction proceeds in a greedy manner. First the set of all data objects are associated with the root. Among all attributes one is chosen to partition the set. The criterion that is applied to select the attribute is based on measuring the <strong>information gain</strong> that can be achieved, or how much uncertainty on the classification of the data objects is removed by the partitioning.</p> <p>Three conditions can occur such that no further partitions can be performed:</p> <p>(1) all data objects are in the same class, therefore further splitting makes no sense,</p> <p>(2) no attributes are left which can be used to split. Still data objects from different classes can be in the leaf, then majority voting is applied.</p> <p>(3) no data objects are left.</p> <h3 id="example">Example</h3> <p><img src="https://raw.githubusercontent.com/lialittis/lialittis.github.io/master/assets/img/example_decision_tree.png" alt=""/></p> <p>Based on this approach for attribute selection, we can now illustrate the induction of the decision tree. In a first step, age is chosen for a split. The partition 31..40 contains after the split only instances from one class, the positive class, thus for this branch of the tree the induction terminates.</p> <p>For the partition &lt;= 30 we find that the student attribute is the best to be chosen for further splitting. Further splitting makes no more sense, as the two resulting partitions, after splitting by the student attribute, are consisting of either positive or negative instances only.</p> <p>Similarly, for the partition &gt;40 we find that credit rating gives the largest information gain. As before, further splitting is no more needed, as the resulting partitions contain only positive respectively negative instances.</p> <h2 id="attribute-selection">Attribute Selection</h2> <h3 id="entropy">Entropy</h3> <p>At a given branch in the tree, the set of samples S to be classified has P positive and N negative instances</p> <p>The entropy of the set S is</p> \[H(P,N) = -\frac{P}{P+N}log_2\frac{P}{P+N} - \frac{N}{P+N}log_2\frac{N}{P+N}\] <ul> <li>Note:</li> </ul> <p>If P = 0 or N = 0, then H(P,N) = 0 -&gt; no uncertainty</p> <p>If P = N, then H(P,N) = 1 -&gt; maximal uncertainty</p> <blockquote> <p>The approach is based on an information-theoretic argument. Assuming that we have a binary category, i.e., two classes P and N to which objects in S have to be assigned, we can compute the amount of information required to determine the class, by H(P, N), the standard entropy measure, where P and N denote the cardinalities of the classes P and N. Given an attribute A that can be used for partitioning the data collection, we can calculate the amount of information needed to classify the data after the split according to attribute A has been performed. This value is obtained by calculating H(P, N) for each of the partitions and weighting these values by the probability that a data item belongs to the respective partition.</p> </blockquote> <h4 id="example-1">Example</h4> <p><img src="https://raw.githubusercontent.com/lialittis/lialittis.github.io/master/assets/img/example_attribute_selection.png" alt=""/></p> <p>We illustrate the attribute selection process now for our running example. Initially the data contains P = 9 positive instances and N = 5 negative instances. This results in an entropy of 0.94, i.e. 0.94 bits are required to decide the class of one instance. We compute next the entropies of all partitions that result from splitting all attributes. For example, if we split for Age, we obtain 3 partitions, each with a different distribution of positive and negative instances; and thus with different entropies.</p> <h3 id="information-gain">Information Gain</h3> <p>The information gained by a split can thus be determined as the difference of the amount of information needed for correct classification before and after the split. Thus we calculate the reduction in uncertainty that is obtained by splitting according to attribute A and select among all possible attributes the one that leads to the highest reduction. For our example we can conclude that it is best to split on attribute age.</p> <p>Next we compute the weighted sum of all entropies of the partitions in a split. The weights correspond to the probability of an instance falling into an element of the partition. Computing this for all attributes shows, that the attribute H results in the lowest entropy, i.e., leaves the lowest remaining uncertainty about the class membership of instances after the split.</p> <ul> <li>Computation</li> </ul> <p>Attribute A partitions S into $S_1, S_2, … S_v$ Entropy of attribute A is \(H(A) = \sum_{i=1}^v\frac{P_i+N_i}{P+N}H(P_i,N_i)\)</p> <p>The information gain obtained by splitting S using A is \(Gain(A) = H(P,N) - H(A)\) Choose the biggest information gain as the splited one.</p> <h3 id="q--a">Q &amp; A</h3> <p>Given the distribution of positive and negative samples for attributes A1 and A2, which is the best attribute for splitting?</p> <table> <thead> <tr> <th>A1</th> <th>P</th> <th>N</th> </tr> </thead> <tbody> <tr> <td>a</td> <td>2</td> <td>2</td> </tr> <tr> <td>b</td> <td>4</td> <td>0</td> </tr> </tbody> </table> <table> <thead> <tr> <th>A2</th> <th>P</th> <th>N</th> </tr> </thead> <tbody> <tr> <td>x</td> <td>3</td> <td>1</td> </tr> <tr> <td>y</td> <td>3</td> <td>1</td> </tr> </tbody> </table> <p>Answer:[<font color:white="">A1</font>]</p> <h2 id="pruning-修剪">Pruning [修剪]</h2> <blockquote> <p>A common problem in classification is that a classifier may <strong>overspecialize and capture noise and outliers in the data</strong>[噪声和离群值对数据的影响], rather than <strong>general properties</strong>. One possibility to limit overspecialization would be to <strong>stop the partitioning of tree nodes when some specific criteria is met</strong> (e.g., number of samples assigned to the leaf node). A possible criterion is to <strong>stop partitioning when the majority of remaining samples falls into one class</strong>. However, in general it is difficult to specify a suitable criterion a priori (e.g. choosing the right value of epsilon).</p> </blockquote> <blockquote> <p>Another alternative is to <strong>first build the complete classification tree</strong>, and then, in a second phase, <strong>prune subtrees that do not contribute to an efficient classification</strong>. Different approaches can be applied to that end: <strong>heuristic approaches[启发式方法] can identify subtrees that do not contribute to the classification accuracy, and eliminate those</strong>. A more principled approach is <strong>the use of the minimum description length principle</strong>. (MDL).</p> </blockquote> <p>The construction phase does not filter out noise → <strong>overfitting</strong></p> <p>Pruning strategies</p> <ul> <li> <p>Stop partitioning a node when large majority of samples is positive or negative, i.e., 𝑁/(𝑁+𝑃) or 𝑃/(𝑁+𝑃)&gt;1−𝜀</p> </li> <li> <p>Build the full tree, then replace nodes with leaves labelled with the majority class, if classification accuracy does not change</p> </li> <li> <p>Apply Minimum Description Length (MDL) principle</p> </li> </ul> <p>分类算法： https://blog.csdn.net/china1000/article/details/48597469</p> <h3 id="minimum-description-length-pruning">Minimum Description Length Pruning</h3> <p>The MDL is based on the following consideration: if the effort in order to specify a class (the implicit description of the class extension through a decision tree) exceeds the effort to enumerate all class members (the explicit description of the class by enumerating its extension), then the subtree is over classifying and non-optimal. To measure the description cost a suitable metrics for the encoding cost, both for trees and data sets is required. For trees this can be done by suitably counting the various structural elements needed to encode the tree (#nodes, #test predicates, # arcs), whereas for explicit classification, it is sufficient to count the number of misclassifications that occur in a tree node.</p> <p>补充： 剪枝的目的是为了通过删除部分节点和子树，以避免“过度合适”/“过学习”，因为过学习将导致我们作出的假设的泛化能力过差。最小描述长度（MDL）准则表示，解释一组数据最好的理论，应该使下面这两项之和最下：</p> <ol> <li>描述理论所需要的比特长度；</li> <li>在理论的协助下，对数据编码所需要的比特长度。</li> </ol> <p>而决策树中的MDL应当是使得训练样本的大多数数据符合这棵树，其他的样本作为例外编码，使得下面这两项最小：</p> <ol> <li>编码决策树所需要的比特，它代表了猜想</li> <li>编码例外实例所需要的比特</li> </ol> <p>Let M1, M2, …, Mn be a list of candidate models (i.e., trees). The best model is the one that minimizes \(L(M) + L(D|M)\) where</p> <ul> <li> <p>L(M) is the length of the description of the model in bits (#nodes, #leaves, #arcs …)</p> </li> <li> <table> <tbody> <tr> <td>L(D</td> <td>M) is the is the length of the description of the data when encoded with the model in bits (#misclassifications)</td> </tr> </tbody> </table> </li> </ul> <h2 id="continuous-attributes">Continuous Attributes</h2> <p>With continuous attributes it does not make sense to create a separate path in the decision tree for every possible attribute value. Instead, in such a case, a binary decision tree is constructed. Binary decisions can be specified both for continuous and categorical attributes. For continuous attributes, the binary split is performed by selecting a threshold that separates the instances in those that have a larger and a smaller value than the threshold. For categorical attributes, a subset of attribute values can be chosen that distinguishes the instances in two subsets.</p> <p>With continuous attributes we cannot have a separate branch for each value</p> <ul> <li>use binary decision trees</li> </ul> <p>Binary decision trees</p> <ul> <li>For continuous attributes A a split is defined by val(A) &lt; X</li> <li>For categorical attributes A a split is defined by a subset X $\subseteq$ domain(A)</li> </ul> <h3 id="example-2">Example</h3> <p>This example shows a dataset with both categorical and continuous attributes and a possible binary decision tree for such a dataset. The class label in the example is Risk.</p> <p><img src="https://raw.githubusercontent.com/lialittis/lialittis.github.io/master/assets/img/example_binary_bt.png" alt=""/></p> <h3 id="splitting-continuous-attributes">Splitting Continuous Attributes</h3> <p>Approach</p> <ul> <li>Sort the data according to attribute value</li> <li>Determine the value of X which maximizes information gain by scanning through the data items</li> </ul> <p>Only if the class label changes, a relevant decision point exists</p> <blockquote> <p>When splitting the dataset using a continuous attribute, we need to determine which is the optimal value to split the dataset based on this attribute. To that end, first the set of attribute values is sorted. Then the class labels are traversed, and whenever it changes a possible split point is found (it can be shown that splitting where class labels do not change is provably sub-optimal). At these points the information gain needs to be computed.</p> </blockquote> <h4 id="scalability-of-continuous-attributes">Scalability of Continuous Attributes</h4> <p>Naive implementation</p> <ul> <li>At each step the data set is split in subsets that are associated with a tree node</li> </ul> <p>Problem</p> <ul> <li>For evaluating which continuous attribute to split, data needs to be sorted according to these attributes</li> <li>Becomes dominating cost</li> </ul> <p>In a naïve implementation of the splitting process, we would keep all data in a single table. This would imply that we would have for traversing the attributes in order to resort that table every time an attribute is investigated. Therefore, a more efficient approach is needed.</p> <blockquote> <p>To avoid repeated resorting of data, for every attribute a separate and presorted table is kept. Once a split is chosen, we find two different situation. For the table that keeps the attribute that was used in the split, the table needs just to be partitioned into two subtables, maintaining the order. For the other attributes we have to select the subtables corresponding to the instances of the two partitions that have been formed. To that end a temporary hash table is constructed that allows to associate to each dataitem its partition. Then the attribute table is scanned and partitioned using the hashtable to decide for each entry to which partition it belongs. Note that in this approach, for continuous attributes, the resulting tables are again sorted as the order is preserved from the original table.</p> </blockquote> <p><strong>Idea: Presorting of data and maintaining order throughout tree construction</strong></p> <ul> <li>Requires separate sorted attribute tables for each attribute</li> </ul> <p>Updating attribute tables</p> <ul> <li>Attribute used for split: splitting attribute table straightforward</li> <li>Other attributes <ul> <li>Build Hash Table once associating tuple identifiers (TIDs) of data items with partitions</li> <li>Select data from other attribute tables by scanning and probing the hash table</li> </ul> </li> </ul> <h3 id="example-3">Example</h3> <p><img src="https://raw.githubusercontent.com/lialittis/lialittis.github.io/master/assets/img/example_continuous_splits.png" alt=""/></p> <p>In this example we demonstrate of how attribute tables are split, when a decision node is introduced in the decision tree. Since the split is based on attribute Age, the table for Age can simply be split into two subtables at the threshold value. For the Car Type table we use the temporary hash table indicating partition membership to separate it into two subtables.</p> <h3 id="q--a-1">Q &amp; A</h3> <p>When splitting a continuous attribute, its values need to be sorted …</p> <p>A. to avoid overfitting</p> <p>B. to prune the data</p> <p>C. to define a binary split condition</p> <p>D. to accelerate tree induction</p> <p>Answer:[<font color:white="">C</font>]</p> <h2 id="characteristics-of-decision-tree-induction">Characteristics of Decision Tree Induction</h2> <p>We summarize here some of the major strengths and weaknesses of standard decision tree induction.</p> <p><strong>Decision trees advantages</strong></p> <ul> <li>The information theoretic criteria used to select the most discriminative attribute is an embedded feature selection</li> <li>No data preparation is needed, such as normalisation of data</li> <li>The best aspect of using trees for analytics is that they are easy to interpret and explain, while more sophisticated ML algorithms (ANN, SVM) are seen as black-boxes that do not “explain” the classification decisions they make</li> </ul> <p><strong>Decision trees drawbacks</strong></p> <ul> <li>They can be extremely sensitive to small perturbations in the data: a slight change can result in a drastically different tree.</li> <li>They can easily overfit. This can be compensated by validation methods and pruning, but remains a problem.</li> <li>They are not incremental. If new data is available, the existing tree cannot be incrementally modified, but the whole tree must be reconstructed from scratch</li> </ul> <hr/> <p>Strengths</p> <ul> <li>Automatic feature selection</li> <li>Minimal data preparation</li> <li>Non-linear model</li> <li>Easy to interpret and explain</li> </ul> <p>Weaknesses</p> <ul> <li>Sensitive to small perturbation in the data</li> <li>Tend to overfit</li> <li>No incremental updates</li> </ul> <hr/> <h3 id="decision-tree-induction-properties">Decision Tree Induction: Properties</h3> <p>Model: flow-chart like tree structure</p> <p>Score function: classification accuracy</p> <p>Optimisation: top-down tree construction + pruning</p> <p>Data Management: avoiding sorting during splits</p> <h2 id="classification-algorithms">Classification Algorithms</h2> <p>Decision trees is one of the best known and historically first examples of a classification approach. Many other methods have been devised in studied over tie. These include basic methods (we will see some examples later), ensemble methods (discussed in the following), support vector machines (a paradigm based on splitting the space through hyper-planes), and neural networks (which are attracting recently significant attention and are nowadays among the best performing classifiers if very large training sets are available).</p> <p>Decision tree induction is a (well-known) example of a classification algorithm</p> <p>Alternatives</p> <ul> <li>Basic methods: Naïve Bayes, kNN, logistic regression, ..</li> <li>Ensemble methods: random forest, gradient boosting, …</li> <li>Support vector machines</li> <li>Neural networks: CNN, rNN, LSTM, …</li> </ul> <h3 id="ensemble-methods">Ensemble Methods</h3> <p>One important development in decision trees was the introduction of the idea of ensemble methods. The basic principle is simple: instead of constructing a single model, many different models are constructed independently. Even if each model is not very expressive (weak learners) their combination can be powerful (strong learner). Different ensemble methods are distinguished by the type of approach they are based on. Ensemble methods, which we will discuss in the following, learn several models in parallel, and combine then their predictions by voting or averaging. Stacking methods use more sophisticated techniques to combine model outputs, based themselves on learning methods. Finally, boosting learn models in sequence. In each step the samples of the training data are reweighted depending on whether they have been correctly classified.</p> <p>Idea</p> <ul> <li>Take a collection of simple or weak learners</li> <li>Combine their results to make a single, strong learner Types</li> <li>Bagging: train learners in parallel on different samples of the data, then combine outputs through voting or averaging</li> <li>Stacking: combine model outputs using a second-stage learner like linear regression[线性回归]</li> <li>Boosting: train learners on the filtered output of other learners</li> </ul> <h3 id="random-forests-algorithm">Random Forests: Algorithm</h3> <p>Random forests are an ensemble method based on bagging. The principle is very simple: K different decision trees are learnt in parallel from different (independent) samples of the data, and the classification is derived from a majority vote of the predictions.</p> <p>Learn K different decision trees from independent samples of the data (bagging); vote between different learners, so models should not be too similar</p> <p>Aggregate output: majority vote</p> <h4 id="why-do-ensemble-methods-work">Why do Ensemble Methods Work?</h4> <p>Here we give the argument why ensemble methods work: even if the individual classifiers are not very good (e.g. make 35% errors in prediction) their aggregate will be very strong. For example, if we have 25 classifiers, the probability that a majority of them, namely at least 13, make a wrong prediction is very small, namely 6%. In general, ensemble methods work well, if the individual models are better than random guessing. The figure illustrates the relation between the classification errors of individual classifiers and the aggregate classification accuracy.</p> <h4 id="sampling-strategies">Sampling Strategies</h4> <p>Two sampling strategies</p> <p>Sampling data</p> <ul> <li>select a subset of the data → Each tree is trained on different data</li> </ul> <p>Sampling attributes</p> <ul> <li>select a subset of attributes → corresponding nodes in different trees (usually) don’t use the same feature to split</li> </ul> <p>For random forests the main issue is the choice of the sampling strategy, i.e., the generation of samples that are used for learning the individual models. Specifically, it consists of two different sampling strategy. The first, sampling data selects from the original dataset a sample. Thus each decision tree is trained on a different sample of data. The second, sampling attributes selects from the attributes available to take a decision a random subset. Thus even if a tree would have been constructed in the same way up to a level, the continuation might become different due to attribute sampling (e.g. the optimal attribute in one tree is not available for splitting in the other tree).</p> <h4 id="algorithm-context">Algorithm context</h4> <ol> <li>Draw K bootstrap samples of size N from original dataset, with replacement (bootstrapping)</li> <li>While constructing the decision tree, select a random set of m attributes out of the p attributes available to infer split (feature bagging)</li> </ol> <p>Typical parameters</p> <ul> <li>m ≈ sqrt(p), or smaller</li> <li>K ≈ 500</li> </ul> <h4 id="illustration-of-random-forests">Illustration of Random Forests</h4> <p><img src="https://raw.githubusercontent.com/lialittis/lialittis.github.io/master/assets/img/illustration_random_forests.png" alt=""/></p> <p>Random forests allow to learn much more complex functions than basic decision trees. This fact is illustrated in this visualization. For the same training data set different numbers of decision trees are constructed (rCART is a variant of decision trees). We observe that with increasing numbers of trees the decision boundaries become increasingly more complex and smoother, and thus a better separation among different classes can be achieved.</p> <h3 id="q--a-2">Q &amp; A</h3> <p>The computational cost for constructing a RF with K as compared to constructing K decision trees on the same data</p> <p>A. is identical</p> <p>B. is on average larger</p> <p>C. is on average smaller</p> <p>Answer:[<font color:white="">C</font>]</p> <h3 id="characteristics-of-random-forests">Characteristics of Random Forests</h3> <p>Random forests are a very popular method for classification due to the many advantages they offer. They are considered as the method of choice in cases where the data is dense, which means that the number of features is relatively low (in the thousands). Sparse data would be, for example, vector space representation of documents with very large vocabularies. In such cases, before applying a method such as random forests, a dimensionality reduction would have to be applied. This could be accomplished in the case of documents by creating a word embedding.</p> <p>Strengths</p> <ul> <li>Ensembles can model extremely complex decision boundaries <strong>without overfitting</strong></li> <li>Probably the most popular classifier for <strong>dense data</strong> (≤ a few thousand features)</li> <li><strong>Easy to implement</strong> (train a lot of trees)</li> <li><strong>Parallelizes easily, good match for MapReduce</strong></li> </ul> <p>More recently, in cases where large training sets are available or number of features is very large, deep neural networks exhibit better performance than random forests.</p> <p>Weaknesses</p> <ul> <li>Deep Neural Networks generally do better</li> <li>Needs many passes over the data – at least the max depth of the trees</li> <li>Relatively easy to overfit – hard to balance accuracy/fit tradeoff</li> </ul> <h2 id="references">references</h2> <p>Textbook</p> <ul> <li>Jiawei Han, Data Mining: concepts and techniques, Morgan Kaufman, 2000, ISBN 1-55860-489-8 References</li> <li>Leo Breiman (2001) “Random Forests” Machine Learning, 45, 5-32. </li> <li>Shafer, John, Rakesh Agrawal, and Manish Mehta. “SPRINT: A scalable parallel classifier for data mining.” Proc. 1996 Int. Conf. Very Large Data Bases. 1996.</li> </ul>]]></content><author><name>TC YU</name></author><category term="Data Mining"/><category term="Distributed Information System"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Software Security 1 (Basic Principles)</title><link href="https://lialittis.github.io/blog/2020/Software-Security-1-(Basic-Principles)/" rel="alternate" type="text/html" title="Software Security 1 (Basic Principles)"/><published>2020-02-19T00:00:00+00:00</published><updated>2020-02-19T00:00:00+00:00</updated><id>https://lialittis.github.io/blog/2020/Software%20Security%201%20(Basic%20Principles)</id><content type="html" xml:base="https://lialittis.github.io/blog/2020/Software-Security-1-(Basic-Principles)/"><![CDATA[<h1 id="software-security-1-basic-principles">Software Security 1 (Basic Principles)</h1> <p><strong>Mathias Payer</strong> EPFL Spring 2020</p> <p>Allow intended use of software, prevent unintended use that may cause harm.</p> <h2 id="security-principles--cia-security-triad">Security principles / CIA security triad</h2> <ul> <li><strong>Confidentiality</strong>: an attacker cannot recover protected data</li> <li><strong>Integrity</strong>: an attacker cannot modify protected data</li> <li><strong>Availability</strong>: an attacker cannot stop/hinder computation</li> </ul> <p>Accountability/non-repudiation may be used as fourth fundamental concept. It prevents denial of message transmission or receipt.</p> <h2 id="attacks-and-defenses">Attacks and Defenses</h2> <ul> <li>Attack (threat) models <ul> <li>A class of attacks that you want to stop</li> <li>What is the attacker’s capability?</li> <li>What is impact of an attack?</li> <li>What attacks are out-of-scope?</li> </ul> </li> <li>Defenses address a certain attack/threat model <ul> <li>General: e.g., stop memory corruptions</li> <li>Very specific: e.g., stop overwriting a return address</li> </ul> </li> </ul> <h3 id="threat-model">Threat model</h3> <p>The threat model defines the abilities and resources of the attacker. Threat models enable structured reasoning about the attack surface.</p> <ul> <li>Awareness of entry points (and associated threats)</li> <li>Look at systems from an attacker’s perspective <ul> <li>Decompose application: identify structure</li> <li>Determine and rank threats</li> <li>Determine counter measures and mitigation</li> </ul> </li> </ul> <p>Further reading: https://www.owasp.org/index.php/Application_Threat_Modeling</p> <p><strong>Threat model for a safe/lockbox</strong></p> <p>You want to protect your valuables by locking them in a safe.</p> <p>In trust land, you don’t need to lock your safe. An attacker may pick your lock. An attacker may use a torch to open your safe. An attacker may use advanced technology (x-ray) to open it. An attacker may get access (or copy) your key.</p> <p><strong>Threat model: operating systems</strong></p> <p>Malicious extension【恶意扩展】: attacker-controlled driver injected in OS;在操作系统中加入攻击者控制的驱动程序<br/> Bootkit: compromise the boot process (BIOS, boot sectors)危及boot进程;<br/> Memory corruption: software bugs such as spatial and temporal memory safety errors or hardware bugs such as rowhammer;<br/> Data leakage【数据泄露】: the OS accidentally returns confidential data (e.g., randomization secrets);<br/> Concurrency bugs【并发问题】: unsynchronized【不同步】 reads across privilege levels result in TOCTTOU (time of check to time of use【检查时间到使用时间】) bugs;<br/> Side channels: indirect data leaks through shared resources such as hardware (e.g., caches), speculation (Spectre or Meltdown), or software (page deduplication);<br/> Malicious peripherals【恶意外围设备】: an attacker may connect malicious peripherals to an exposed bus;<br/> Resource depletion and deadlocks: stop legitimate computation【合法计算】 by exhausting【耗尽】 or blocking【锁死】 access to resources</p> <ul> <li><strong>Cost of security</strong></li> </ul> <p>There is no free lunch, security incurs overhead. Security is. . . expensive to develop, expensive to maintain, may have performance overhead, may be inconvenient to users.</p> <ul> <li><strong>Fundamental security mechanisms</strong></li> </ul> <ol> <li>Isolation</li> </ol> <p>Isolate two components from each other. One component cannot access data/code of the other component except through a well-defined API.</p> <ol> <li>Least privilege</li> </ol> <p>The principle of least privilege ensures that a component has the least privileges needed to function.</p> <p>最小特权原则是系统安全中最基本的原则之一。所谓最小特权(Least Privilege)，指的是”在完成某种操作时所赋予网络中每个主体(用户或进程)必不可少的特权”。最小特权原则，则是指”应限定网络中每个主体所必须的最小特权，确保可能的事故、错误、网络部件的篡改等原因造成的损失最小”。</p> <p>Any further removed privilege reduces functionality Any added privilege will not increase functionality (according to the specification) This property constrains an attacker in the obtainable privileges</p> <p>最小特权原则一方面给予主体”必不可少”的特权，这就保证了所有的主体都能在所赋予的特权之下完成所需要完成的任务或操作;另一方面，它只给予主体”必不可少”的特权，这就限制了每个主体所能进行的操作。 最小特权原则要求每个用户和程序在操作时应当使用尽可能少的特权，而角色允许主体以参与某特定工作所需要的最小特权去签入(Sign)系统。被授权拥有强力角色(Powerful Roles)的主体，不需要动辄运用到其所有的特权，只有在那些特权有实际需求时，主体才去运用它们。如此一来，将可减少由于不注意的错误或是侵入者假装合法主体所造成的损坏发生，限制了事故、错误或攻击带来的危害。它还减少了特权程序之间潜在的相互作用，从而使对特权无意的、没必要的或不适当的使用不太可能发生。这种想法还可以引申到程序内部：只有程序中需要那些特权的最小部分才拥有特权。</p> <ol> <li>Fault compartments 【故障舱】</li> </ol> <p>Separate individual components into smallest functional entity possible. These units contain faults to individual components. Allows abstraction and permission checks at boundaries.</p> <p>Note that this property builds on least privilege and isolation. Both properties are most effective in combination: many small components that are running and interacting with least privileges.</p> <ul> <li>Example: mail server</li> </ul> <p><strong>Mail Transfer Agents (MTA)</strong> need to do a plethora of tasks:</p> <blockquote> <p>Send/receive data from the network</p> </blockquote> <blockquote> <p>Manage a pool of received/unsent messages</p> </blockquote> <blockquote> <p>Provide access to stored messages for each user</p> </blockquote> <p>-&gt; Two approaches: sendmail and qmail</p> <p>Sendmail uses a typical Unix approach with a large monolithic【庞大的】 server and is known for the high complexity and previous security vulnerabilities【脆弱点】.</p> <p>QMail uses a modern least privilege approach with a set of communicating processes.</p> <p><strong>QMail</strong> [详见ppt]</p> <blockquote> <p>qmail is a mail transfer agent (MTA) that runs on Unix. It was written, starting December 1995, by Daniel J. Bernstein as a more secure replacement for the popular Sendmail program. Originally license-free software, qmail’s source code was later dedicated in the public domain by the author. When first published, qmail was the first security-aware mail transport agent; since then, other security-aware MTAs have been published. The most popular predecessor to qmail, Sendmail, was not designed with security as a goal, and as a result has been a perennial target for attackers. In contrast to sendmail, qmail has a modular architecture composed of mutually untrusting components; for instance, the SMTP listener component of qmail runs with different credentials from the queue manager or the SMTP sender. qmail was also implemented with a security-aware replacement to the C standard library, and as a result has not been vulnerable to stack and heap overflows, format string attacks, or temporary file race conditions.</p> </blockquote> <p>Separate modules run under separate user IDs <strong>(isolation)</strong> Each user ID has only limited access to a subset of the resources <strong>(least privilege)</strong> Only one very small component runs as suid root Only one very small component running as root</p> <blockquote> <p>SUID (Set owner User ID up on execution) is a special type of file permissions given to a file. Normally in Linux/Unix when a program runs, it inherit’s access permissions from the logged in user. SUID is defined as giving temporary permissions to a user to run a program/file with the permissions of the file owner rather that the user who runs it. In simple words users will get file owner’s permissions as well as owner UID and GID when executing a file/program/command.</p> </blockquote> <ol> <li>Trust and correctness</li> </ol> <p>Specific components are assumed to be trusted or correct according to a specification</p> <p>Formal verification ensures that a component correctly implements a given specification and can therefore be trusted. <strong>Note that this property is an ideal property that cannot generally be achieved.</strong></p> <h2 id="hardware-and-software-abstractions">Hardware and software abstractions</h2> <h3 id="operating-system-os-abstractions">Operating System (OS) abstractions</h3> <ul> <li>Provides process abstraction</li> <li>Well-defined API to access hardware resources</li> <li>Enforces mutual exclusion to resources</li> <li>Enforces access permissions for resources</li> <li>Restrictions based on user/group/ACL</li> <li>Restricts attacker</li> </ul> <p>OS process isolation:</p> <ul> <li>Memory protection :protect the memory (code and data such as heap, stack, or globals) of one process from other processes;</li> <li>Address space: working memory of one process</li> </ul> <p>Today’s system implement address spaces (virtual memory) through page tables with the help of an Memory Management Unit (MMU)</p> <ol> <li><strong>Single domain OS (Hardware -&gt; Application)</strong> <ol> <li>A single layer, no isolation or compartmentalization</li> <li>All code runs in the same domain: the application can directly call into operating system drivers</li> <li>High performance, often used in embedded systems</li> </ol> </li> <li><strong>Monolithic OS(Hardware -&gt; -&gt;several applications)</strong> <ol> <li>Two layers: the operating system and applications</li> <li>The OS manages resources and orchestrates access</li> <li>Applications are unprivileged, must request access from the OS</li> <li>Linux fully and Windows mostly follows this approach for performance (isolating individual components is expensive)</li> </ol> </li> <li><strong>Micro-kernel</strong> <ol> <li>Many layers: each component is a separate process</li> <li>Only essential parts are privileged <ol> <li>Process abstraction (address spaces)</li> <li>Process management (scheduling)</li> <li>Process communication (IPC)</li> </ol> </li> <li>Applications request access from different OS processes</li> </ol> </li> <li><strong>Library OS</strong> <ol> <li>Few thin layers; flat structure</li> <li>Micro-kernel exposes bare OS services</li> <li>Each application brings all necessary OS components</li> </ol> </li> </ol> <h3 id="hardware-abstractions">Hardware abstractions</h3> <p>Virtual memory through MMU/OS</p> <p>Only OS has access to raw physical memory</p> <p>DMA for trusted devices</p> <p>ISA enforces privilege abstraction (ring 0/3 on x86)</p> <p>Hardware abstractions are fundamental for performance</p> <h2 id="access-control">Access control</h2> <ul> <li>Authentication【证明】: Who are you (what you know, have, or are)?</li> <li>Authorization【授权】: Who has access to object?</li> <li>Audit/Provenance【审查，来源】: I’ll check what you did.</li> </ul> <h3 id="authentication">Authentication</h3> <p>There are three fundamental types of identification:</p> <ol> <li>What you know: username / password</li> <li>What you are: biometrics</li> <li>What you have: smartcard / phone</li> </ol> <p>Identification is stronger in combination:</p> <ol> <li>two factors are better than one</li> <li>Check for username/password and presence of a security token</li> </ol> <h3 id="authorization">Authorization</h3> <p>—— Information Flow Control (IFC)</p> <p>Who can access what information?</p> <ol> <li>Access policies are called <strong>access control models</strong>.</li> <li>Access control models originally developed by the US military <ul> <li>Users with different clearance levels【许可级别】 on a single system</li> <li>Data was shared across different levels of clearance</li> <li>Therefore the name “multi-level security”</li> </ul> </li> </ol> <p>—— Types of access control</p> <ol> <li>Mandatory Access Control (MAC) 【强制/命令访问控制】 <strong>Idea: rule and lattice-based policy</strong><br/> <ul> <li>Centrally controlled</li> <li>One entity controls what permissions are given</li> <li>Users cannot change policy themselves <blockquote> <p>Examples: Bell/LaPadula and Biba\</p> </blockquote> </li> </ul> </li> </ol> <p><strong>Bell-LaPadula模型 (BLP)</strong> 是一种状态机模型，用于在政府和军事应用中实施访问控制。它是由David Elliott Bell 和Leonard J. LaPadula在Roger R. Schell的指导下设计的，用于规范美国国防部 (DoD) 的多级安全 (MLS) 策略。该模型是计算机安全策略的形式状态转换模型，它利用访问主体和访问对象的安全等级来描述一系列访问控制规则。安全等级的范围从最敏感（如“最高机密”）到最不敏感（如“未分类”或“公开”）。Bell-LaPadula模型是保护与安全之间没有明显区别的模型之一。<br/> 在存在受信任主体的情况下，Bell-LaPadula模型可能会产生从高机密文档到低机密文档的信息流动。受信任主体不受*属性的限制，但必须证明其在安全策略方面是值得信任的。该安全模型针对访问控制，并被描述为：“下读，上写”。<br/> 在Bell-LaPadula模型中，用户只能在其自己的安全级别或更高的安全级别上创建内容（如，秘密研究人员可以创建秘密或绝密文件，但不能创建公共文件；不能下写）。相反，用户只能查看在其自己的安全级别或更低的安全级别的内容（如，秘密研究人员可以查看公共或秘密文件，但不能查看绝密文件；不能上读）。\</p> <p>Multi level security model that enforces information flow control;<br/> All security levels are monotonically ordered;<br/> -&gt; Objects (files) have a clearance level【许可级别】<br/> A given clearance allows reading objects of lower or equal clearance and writing files of equal or higher clearance;<br/> Summarized as read-down, write-up;<br/> Bell/LaPadula enforces confidentiality;\</p> <p>注重机密性，此模型强调就是保证机密不能泄密。所以，机密程度越高，越保密，低的就不能读。写方面，高不能往低写，写的话机密的东西流到机密低的地方了。 所以他不允许数据由高级往低级流，但允许低级往高级流，机密性低的数据，跑到机密性高的地方，并不破坏机密性。 比如，1，2，3，4 ， 1级为最高机密，那么2级的人，不可以读1级数据，这将破坏保密原则。所以不能向上读。反之，1级的人，向2级区域写东西，则也会破坏机密性。</p> <p>If implemented naively, attacker may overwrite confidential files. Mathias Payer</p> <p><strong>Biba模型</strong></p> <p>Multi level security model that enforces information flow control;<br/> All security levels are monotonically ordered;<br/> Objects (files) have an integrity level【完整性级别】<br/> A given clearance allows reading files of higher or equal clearance and writing files of lower or equal clearance;<br/> Biba introduces notion of integrity, separate from secrecy;<br/> Summary: read-up, write-down;<br/> Biba enforces integrity;\</p> <p>完整性1，2，3，4级， 1级是最高完整性，比如是董事会最高决议，它并不需要保密，谁都能看，但任何人不得破坏其完整性，不能修改其决议，只有1级的人才能修改。所以2级的人不可以写1级的内容。但2级可以读1级的内容，来生成2级的信息。 此时，数据只能由高向低流，而禁止从低向高流。</p> <p>If implemented naively, attacker may leak privileged information.</p> <p><em>另，状态机模型的介绍另见笔记“有限状态机”。</em></p> <ol> <li>Discretionary【任意的，无条件的】 Access Control (DAC) 【自由访问控制】</li> </ol> <p>Idea: object owner specifies policy</p> <ul> <li>MAC requires central control, empower【许可，准许】 the user!</li> <li>User has authority over her resources (introduce ownership)</li> <li>User sets permissions for her data if other users want access</li> <li>For example: Unix permissions</li> </ul> <p>Can become complex and may require per-file special casing.</p> <ol> <li>Role-Based Access Control (RBAC) 【基于角色访问控制】</li> </ol> <p><strong>Policy defined in terms of roles (sets of permissions), individuals are assigned roles, roles are authorized for tasks.</strong></p> <ul> <li>Access permission is broken into sets of roles.</li> <li>Users get assigned specific roles</li> <li>Administration privileges may be a role.</li> </ul> <p>Roles simplify special casing but roles can become overly complex.</p> <h2 id="access-control-matrix">Access control matrix</h2> <p>map access rights for subjects to objects</p> <table> <thead> <tr> <th style="text-align: center"> </th> <th style="text-align: center">foo</th> <th style="text-align: center">bar</th> <th style="text-align: center">baz</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">user</td> <td style="text-align: center">rwx</td> <td style="text-align: center">rw</td> <td style="text-align: center">rw</td> </tr> <tr> <td style="text-align: center">group</td> <td style="text-align: center">rx</td> <td style="text-align: center">r</td> <td style="text-align: center">r</td> </tr> <tr> <td style="text-align: center">other</td> <td style="text-align: center">rx</td> <td style="text-align: center"> </td> <td style="text-align: center">r</td> </tr> </tbody> </table> <p>Used, e.g., for Unix/Linux file systems or Android/iOS/Java security model for privileged APIs.</p> <p>Introduced by Butler Lampson in 1971 http://research.microsoft.com/enus/um/people/blampson/08-Protection/Acrobat.pdf</p> <h2 id="summary">Summary</h2> <ul> <li>Software security goal: allow intended use of software, prevent unintended use that may cause harm.</li> <li>Security triad, core principles: Confidentiality, Integrity, Availability.</li> <li>Security of a system depends on its threat model.</li> <li>Concepts: isolation, least privilege, fault compartments, trust.</li> <li>Security relies on abstractions to reduce complexity.</li> <li>Reading assignment: Butler Lampson, Protection http://doi.acm.org/10.1145/775265.775268</li> </ul>]]></content><author><name>TC YU</name></author><category term="Software Security"/><category term="软件安全"/><summary type="html"><![CDATA[Software Security 1 (Basic Principles)]]></summary></entry><entry><title type="html">Introduction to computational complexity</title><link href="https://lialittis.github.io/blog/2020/Introduction-to-computational-complexity/" rel="alternate" type="text/html" title="Introduction to computational complexity"/><published>2020-01-04T00:00:00+00:00</published><updated>2020-01-04T00:00:00+00:00</updated><id>https://lialittis.github.io/blog/2020/Introduction%20to%20computational%20complexity</id><content type="html" xml:base="https://lialittis.github.io/blog/2020/Introduction-to-computational-complexity/"><![CDATA[<head> <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script> </head> <h1 id="introduction-to-combinatorail-optimization">Introduction to combinatorail optimization</h1> <h2 id="some-examples">Some examples:</h2> <ol> <li> <p>Connecting equipments –&gt; minimize the total connections cost [picture:connecting_equipments.jpg] weighted graphs</p> </li> <li> <p>Take-offs planning –&gt; minize the maximal delay on the set of planes</p> </li> </ol> <p>HARD. Scheduling tasks with length and earliest starting time with minimising the maximal delay is a difficult problem(Travelling Salesman Problem)</p> <p>行商问题（最短路径问题）（英语：travelling salesman problem, TSP）是这样一个问题：给定一系列城市和每对城市之间的距离，求解访问每一座城市一次并回到起始城市的最短回路。它是组合优化中的一个NP困难问题，在运筹学和理论计算机科学中非常重要。</p> <ol> <li>Project management –&gt; Prgqnize tqsks in time in order to minimize the project total length</li> </ol> <h2 id="some-common-features">Some common features</h2> <ol> <li>a search space <em>S</em>: the possible alternatives</li> <li>the search space is represented by a set of variables $V = { v_i : i \in { 1,…,n}}$,each variable $v_i$ having a domain $D_i$</li> <li>a set $C_o$ of constraints to be satisfied <ul> <li>constraints are assertions[约束是确定的，不可改变的]</li> <li>they may represent physical(real-world)limitations or user prerequisites[它们可能代表了实际的物理含义或者某些先决条件]</li> <li>a solution is an alternative that satisfies all constraints in $C_o$</li> </ul> </li> <li>a set $C_r$ of criteria to be satisfied as best <ul> <li>a criterion is a function from <em>S</em> to a totally ordered set(R+ for instance). This function has to be minimized or maximized</li> <li>they represent user preferences</li> </ul> </li> </ol> <h2 id="more-examples">More examples</h2> <ol> <li>The knapsack problem: <ul> <li>a set O of objects to put in the knapsack</li> <li>a dimension D to be considered(weight for instance)</li> <li>maximal capacity C w.r.t the dimension for the knapsack</li> <li>for each object o a consumption D0</li> <li>for each object o, its value V0 reflects its importance Objective: to maximize the sum of values of the chosen objects while respecting the capacity of the knapsack</li> </ul> </li> </ol> <p>Model the knapsack problem –&gt;</p> \[**variables** \forall o \in O, P_o \in \{0,1\} -----\{faux,vrai\} **containte** \sum_{o\in O} D_oP_o \leq C **criteria** Maximize \sum_o\in O V_oP_o\] <ol> <li>The Traveling Salesman Problem</li> </ol> <p><strong>Input</strong>\</p> <ul> <li>a set of towns to be visited by a salesman</li> <li>the distances between the towns <strong>Objective</strong><br/> Find a circui tith minimal length visiting each town exactly one time (hamiltonian circuit).</li> </ul> <p>If you have n towns, the size of the space search is (n-1)!</p> <p><strong>SO, are TSP solution enumerable ?</strong></p> <p><strong>OPtimistic hypothesis</strong>: 10e-12s to compute and ecaluate a path number of towns – computation times(s) 10 – 3.63e-6 15 – 1.31 20 – 2432902 $\approx 0.77year$ 30 – 265252859812191058636</p> <p>$265252859812191058636 s \approx 84111130077 millenia = more than 500 times the age of the Universe !$</p> <p>It’s a combinatorial explosion.[组合爆炸，组合展开]</p> <p><strong>How to deal with combinatorial explosion ?</strong></p> <p>The Challenge is: Produce optimal or approached solution without exploring the complete search space.</p> <ol> <li>Dedicated algorithms: efficient, but costly.[专用算法，高效但费用高]</li> <li>Use/adapt existing algorithms and tools.[利用已存在的算法和工具]</li> <li>Use a <strong>generic modelling framework</strong>, with <strong>generic resolution algorithms</strong>.[通用建模框架，通用解决算法]</li> </ol> <p>[picture:some_modelling_framework_and_resolution_methods.jpg]</p> <h1 id="complexity-theory">Complexity Theory</h1> <h2 id="motivation">Motivation</h2> <p>Observation from previous examples:\</p> <ul> <li>some algorithms seen to be more efficient than others</li> <li>for a given problem, some instances seem to be more difficult to solve than others</li> <li>some problems seem to be more difficult than others</li> </ul> <p><strong>Objecitve</strong>\</p> <ul> <li><strong>build a theory that explains and predicts such phenomena</strong></li> <li><strong>the theory must be independent from the languages compilers or machines that are used</strong></li> </ul> <h2 id="algorithms-and-problems">Algorithms and problems</h2> <ul> <li><strong>Problem</strong>: a set of instances with the same structure on which you ask a question</li> <li><strong>Instance</strong>: a problem with data</li> <li><strong>Algorithms</strong>: a procedure taking an instance as input and answering the question of the problem as output</li> </ul> <h2 id="another-important-examplesat">Another important example:SAT</h2> <p>The SAT (for satisfiability) problem is an important problem for complexity theory.</p> <p><strong>Definition(clause)</strong>\</p> <p>Let ${x_1,…,x_n}$ be a set of boolean variables (i.e. $D_{xi} = {true,false}$)</p> <ul> <li>literal[断言，说明]: a variable $x_i$ or the negation of a variable($\neg x_i$)</li> <li>clause[从句，条款]: a disjunction[分离] of literals, e.g. $x_i \vee \neg x_4 \vee x_5$</li> </ul> <p>– &gt;</p> <ul> <li><strong>Problem</strong>: given a finite set of clauses, is the set satisfiable(i.e. can you assign variables to make all the clauses true)?</li> <li><strong>Instance</strong>: is ${x1 \vee \neg x_4 \vee x_5, \neg x_1 \vee \neg x_5$ satisfiable ?</li> <li><strong>Algorithms</strong>: models enumeration, resolution algorithm, DPLL procedure, etc..</li> </ul>]]></content><author><name>TC YU</name></author><category term="Informatique"/><summary type="html"><![CDATA[]]></summary></entry></feed>